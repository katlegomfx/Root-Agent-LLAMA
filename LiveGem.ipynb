{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A1pkoyZb9Jm3"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import ast\n",
        "import glob\n",
        "import importlib.util\n",
        "from IPython import display\n",
        "from typing import Dict, Callable, List\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HghvVpbU0Uap"
      },
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_API_KEY'] = os.getenv('gemini_key')\n",
        "client = genai.Client(http_options= {\n",
        "      'api_version': 'v1alpha'\n",
        "})\n",
        "model_name = \"gemini-2.5-pro-exp-03-25\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wgHJgpV9Zw4E"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger('Live')\n",
        "# logger.setLevel('DEBUG')\n",
        "logger.setLevel('INFO')\n",
        "\n",
        "MD_HEADING = \"#\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EmTKF_DtrY4U"
      },
      "outputs": [],
      "source": [
        "async def handle_tool_call(session, tool_call, custom_functions):\n",
        "  for fc in tool_call.function_calls:\n",
        "    function_name = fc.name\n",
        "    function_args = fc.args\n",
        "    # Find the function object from the list based on the function name\n",
        "    tool_response = types.LiveClientToolResponse(\n",
        "        function_responses=[types.FunctionResponse(\n",
        "            name=fc.name,\n",
        "            id=fc.id,\n",
        "            response={\n",
        "                'result': custom_functions[function_name](**function_args)},\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    print('\\n>>> ', tool_response)\n",
        "    await session.send(input=tool_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CypjqSb-0C-Q"
      },
      "outputs": [],
      "source": [
        "def handle_server_content(server_content):\n",
        "  model_turn = server_content.model_turn\n",
        "  if model_turn:\n",
        "    for part in model_turn.parts:\n",
        "      executable_code = part.executable_code\n",
        "      if executable_code is not None:\n",
        "        display.display(display.Markdown('-------------------------------'))\n",
        "        display.display(display.Markdown(f'``` python\\n{executable_code.code}\\n```'))\n",
        "        display.display(display.Markdown('-------------------------------'))\n",
        "\n",
        "      code_execution_result = part.code_execution_result\n",
        "      if code_execution_result is not None:\n",
        "        display.display(display.Markdown('-------------------------------'))\n",
        "        display.display(display.Markdown(f'```\\n{code_execution_result.output}\\n```'))\n",
        "        display.display(display.Markdown('-------------------------------'))\n",
        "\n",
        "  grounding_metadata = getattr(server_content, 'grounding_metadata', None)\n",
        "  if grounding_metadata is not None:\n",
        "    display.display(\n",
        "        display.HTML(grounding_metadata.search_entry_point.rendered_content))\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8Y00qqZZt5L-"
      },
      "outputs": [],
      "source": [
        "def extract_functions_with_instruction(file_path: str) -> Dict[str, Callable]:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        parsed = ast.parse(file.read(), file_path)\n",
        "\n",
        "    functions = {}\n",
        "    for node in ast.walk(parsed):\n",
        "        if isinstance(node, ast.FunctionDef):\n",
        "            params = [arg.arg for arg in node.args.args]\n",
        "            module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "            spec = importlib.util.spec_from_file_location(\n",
        "                module_name, file_path)\n",
        "            if spec and spec.loader:\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                spec.loader.exec_module(module)\n",
        "                func = getattr(module, node.name, None)\n",
        "                if callable(func):\n",
        "                    functions[node.name] = func\n",
        "    return functions\n",
        "\n",
        "\n",
        "def load_custom_tools(custom_dir: str = os.path.join(\"simple\", \"code\", \"custom\")) -> Dict[str, Callable]:\n",
        "    registry = {}\n",
        "    py_files = glob.glob(os.path.join(custom_dir, \"*.py\"))\n",
        "    for file in py_files:\n",
        "        registry.update(extract_functions_with_instruction(file))\n",
        "    return registry\n",
        "\n",
        "\n",
        "def load_builtin_tools() -> Dict[str, Callable]:\n",
        "    builtin_path = os.path.join(\"simple\", \"code\", \"function_call.py\")\n",
        "    return extract_functions_with_instruction(builtin_path)\n",
        "\n",
        "\n",
        "def get_tool_registry() -> Dict[str, Callable]:\n",
        "    registry = load_builtin_tools()\n",
        "    registry.update(load_custom_tools())\n",
        "    return registry\n",
        "\n",
        "\n",
        "tool_registry = get_tool_registry()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_tools_docs():\n",
        "    \"\"\"Gets list of tools if\"\"\"\n",
        "    return [types.FunctionDeclaration.from_callable(\n",
        "        callable=func, client=client).to_json_dict()\n",
        "        for name, func in tool_registry.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def code_corpus(directory: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Builds a corpus by reading all Python, Typescript and Javascript files in the directory.\n",
        "    \"\"\"\n",
        "    exclude_files = []\n",
        "    exclude_dirs = ['interest', 'pyds', 'backup', 'models', 'sdlc',\n",
        "                    'self_autoCode', 'self_autoCodebase', 'tests', 'to_confirm_tools', 'node_modules', 'gen_ai', 'idea', 'imagev1', 'pretrained', 'prompts', '.next', 'appgen']\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n",
        "        for file in files:\n",
        "            if file.split('.')[-1] in ['py', 'ts', 'tsx', 'js', 'jsx', 'md'] and file not in exclude_files:\n",
        "                file_paths.append(os.path.join(root, file))\n",
        "\n",
        "    corpus = []\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            text = open(file_path, 'r', encoding='utf-8').read()\n",
        "            # corpus.append(f'\\n{MD_HEADING} {file_path}:\\n{text}\\n')\n",
        "            corpus.append({file_path:text})\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'simple\\\\agent_interactions.py': '# ./simple/agent_interactions.py:\\n\"\"\"\\nRefactored Agent Interactions Module\\n\\nThis module runs a Tkinter+Pygame simulation where the agent moves\\nto different target positions (Python code window, tool selection, etc.)\\nand then triggers the corresponding window.\\n\"\"\"\\n\\nimport tkinter as tk\\nimport threading\\nimport traceback\\nimport pygame # type: ignore\\nimport io\\nimport contextlib\\nimport queue  # Import queue for safer cross-thread communication\\nfrom typing import Optional, Tuple\\nfrom simple.code.tool_registry import tool_registry\\n\\nDEFAULT_CODE = \"\"\"def hello_world():\\n    print(\"Hello, World!\")\\n\\nhello_world()\\n\"\"\"\\n\\nAGENT_SPEED = 5\\nWINDOW_SIZE = (640, 480)\\n\\n\\nclass AgentInteractionManager:\\n    def __init__(self):\\n        # Replace globals with instance attributes.\\n        self.desired_position: Optional[Tuple[int, int]] = None\\n        self.mode: str = \"normal\"  # Modes: \"normal\", \"tool_selection\", \"python_code\"\\n        self.tk_root: Optional[tk.Tk] = None\\n        self.selected_tool: Optional[str] = None\\n        self.tool_info_shown: bool = False\\n        self.status_msg: str = \"\"\\n        self.generated_python_code: str = \"\"\\n        self._pygame_running = threading.Event()  # Event to signal pygame loop state\\n        self._tk_queue = queue.Queue()  # Queue for Tkinter updates from Pygame thread\\n\\n        # If no tk_root provided externally, create one internally but keep it hidden.\\n        # This allows AgentExecutor to instantiate this class without needing Tkinter initially.\\n        # if self.tk_root is None:\\n        #     self.tk_root = tk.Tk()\\n        #     self.tk_root.withdraw()\\n        # Defer Tk root creation/assignment until needed or provided externally.\\n\\n    def _ensure_tk_root(self):\\n        \"\"\"Ensures tk_root exists, creating a hidden one if necessary.\"\"\"\\n        if self.tk_root is None:\\n            print(\"Creating internal hidden Tk root.\")\\n            self.tk_root = tk.Tk()\\n            self.tk_root.withdraw()  # Keep it hidden unless managed externally\\n\\n    def _schedule_tk_update(self, callable_func, *args):\\n        \"\"\"Safely schedule a function call in the Tkinter main thread.\"\"\"\\n        self._ensure_tk_root()\\n        self._tk_queue.put((callable_func, args))\\n        # Signal Tkinter thread to process the queue\\n        if self.tk_root:\\n            # Check queue periodically\\n            self.tk_root.after(10, self._process_tk_queue)\\n\\n    def _process_tk_queue(self):\\n        \"\"\"Process pending updates from the queue in the Tkinter thread.\"\"\"\\n        try:\\n            while not self._tk_queue.empty():\\n                callable_func, args = self._tk_queue.get_nowait()\\n                callable_func(*args)\\n        except queue.Empty:\\n            pass  # No more items for now\\n        except Exception as e:\\n            print(f\"Error processing Tkinter queue item: {e}\")\\n\\n    def get_tool_position(self, index: int, total: int) -> Tuple[int, int]:\\n        \"\"\"Compute evenly spaced positions for tools.\"\"\"\\n        x_start, x_end = 50, 590\\n        step = (x_end - x_start) / (total - 1) if total > 1 else 0\\n        x = int(x_start + index * step)\\n        y = 150\\n        return x, y\\n\\n    def update_status_msg(self, msg: str) -> None:\\n        self.status_msg = msg\\n\\n    def set_generated_code(self, code: str) -> None:\\n        self.generated_python_code = code\\n\\n    def launch_game(self) -> None:\\n        \"\"\"Launches the Pygame simulation in a separate thread.\"\"\"\\n        if not self._pygame_running.is_set():\\n            try:\\n                # Disable launch button if it exists and is bound to this manager instance\\n                if hasattr(self, \\'launch_button\\') and self.launch_button:\\n                    self.launch_button.config(state=tk.DISABLED)\\n            except (AttributeError, tk.TclError):\\n                # Ignore if button doesn\\'t exist or window is closed\\n                pass\\n            self._pygame_running.set()  # Signal that pygame is starting/running\\n            game_thread = threading.Thread(target=self.run_game, daemon=True)\\n            game_thread.start()\\n        else:\\n            print(\"Pygame simulation is already running.\")\\n\\n    def stop_game(self):\\n        \"\"\"Signals the Pygame loop to stop.\"\"\"\\n        self._pygame_running.clear()  # Signal pygame to stop\\n        print(\"Attempting to stop Pygame simulation...\")\\n        # Any Tkinter root cleanup should be handled by the owner of the root\\n\\n    def run_game(self) -> None:\\n        \"\"\"Runs the main Pygame simulation loop.\"\"\"\\n        try:\\n            pygame.init()\\n            screen = pygame.display.set_mode(WINDOW_SIZE)\\n            pygame.display.set_caption(\"Agent Interaction Simulation\")\\n            self._set_pygame_icon()\\n            clock = pygame.time.Clock()\\n            python_image = self._load_image(\\n                \"simple/gag/pygame_python_logo.png\", (50, 50))\\n            tool_image = self._load_image(\\n                \"simple/gag/pygame_toolbox.png\", (50, 50))\\n            font = pygame.font.SysFont(\"Arial\", 20)\\n            agent_x, agent_y = 300, 400\\n\\n            while self._pygame_running.is_set():  # Check the event flag\\n                for event in pygame.event.get():\\n                    if event.type == pygame.QUIT:\\n                        self.stop_game()  # Use the stop method\\n                        break  # Exit inner loop\\n\\n                if not self._pygame_running.is_set():  # Check again after event processing\\n                    break\\n\\n                agent_x, agent_y = self._move_agent(agent_x, agent_y)\\n                screen.fill((0, 0, 0))\\n\\n                if self.mode == \"normal\":\\n                    self._render_normal_mode(\\n                        screen, font, python_image, tool_image, agent_x, agent_y)\\n                elif self.mode == \"tool_selection\":\\n                    self._render_tool_selection(screen, font, agent_x, agent_y)\\n                elif self.mode == \"python_code\":\\n                    self._render_python_code(screen, font)\\n\\n                self._draw_agent(screen, font, agent_x, agent_y)\\n\\n                if self.status_msg:\\n                    status_surface = font.render(\\n                        self.status_msg, True, (255, 255, 255))\\n                    screen.blit(status_surface, (50, 450))\\n\\n                pygame.display.flip()\\n                clock.tick(30)\\n\\n        except pygame.error as e:\\n            print(f\"Pygame error occurred: {e}\")\\n            self._pygame_running.clear()  # Ensure flag is cleared on error\\n        finally:\\n            pygame.quit()\\n            print(\"Pygame quit.\")\\n            # Re-enable launch button if it exists\\n            try:\\n                if hasattr(self, \\'launch_button\\') and self.launch_button and self.tk_root:\\n                    # Use schedule_tk_update to safely modify Tkinter widget from this thread\\n                    self._schedule_tk_update(lambda btn: btn.config(\\n                        state=tk.NORMAL), self.launch_button)\\n            except tk.TclError:\\n                # Ignore if window is already closed\\n                pass\\n            # Do not call sys.exit() here; let the main thread manage application exit.\\n\\n    def _set_pygame_icon(self):\\n        try:\\n            icon_path = \"simple/gag/icon.png\"\\n            icon_image = pygame.image.load(icon_path).convert_alpha()\\n            icon_image = pygame.transform.scale(icon_image, (32, 32))\\n            pygame.display.set_icon(icon_image)\\n        except Exception as e:\\n            print(f\"Error loading pygame icon \\'{icon_path}\\': {e}\")\\n\\n    def _load_image(self, path: str, size: Tuple[int, int]) -> Optional[pygame.Surface]:\\n        try:\\n            image = pygame.image.load(path).convert_alpha()\\n            return pygame.transform.scale(image, size)\\n        except Exception as e:\\n            print(f\"Error loading image \\'{path}\\': {e}\")\\n            return None\\n\\n    def _move_agent(self, agent_x: int, agent_y: int) -> Tuple[int, int]:\\n        # Explicit check if there\\'s a destination\\n        if self.desired_position is None:\\n            return agent_x, agent_y\\n\\n        target_x, target_y = self.desired_position\\n        dx = target_x - agent_x\\n        dy = target_y - agent_y\\n\\n        # Move horizontally\\n        if dx > 0:\\n            agent_x = min(agent_x + AGENT_SPEED, target_x)\\n        elif dx < 0:\\n            agent_x = max(agent_x - AGENT_SPEED, target_x)\\n\\n        # Move vertically\\n        if dy > 0:\\n            agent_y = min(agent_y + AGENT_SPEED, target_y)\\n        elif dy < 0:\\n            agent_y = max(agent_y - AGENT_SPEED, target_y)\\n\\n        # Check if destination reached\\n        if (agent_x, agent_y) == (target_x, target_y):\\n            self.desired_position = None  # Arrived\\n\\n        return agent_x, agent_y\\n\\n    def _render_normal_mode(self, screen, font, python_image, tool_image, agent_x, agent_y):\\n        python_target_pos = (50, 100)\\n        tool_target_pos = (500, 100)\\n\\n        # Render Python button.\\n        python_target_rect = pygame.Rect(*python_target_pos, 50, 50)\\n        if python_image:\\n            screen.blit(python_image, python_target_rect.topleft)\\n        else:  # Fallback rendering\\n            pygame.draw.rect(\\n                screen, (0, 0, 255), (python_target_rect.x, python_target_rect.y, 25, 50))\\n            pygame.draw.rect(\\n                screen, (255, 255, 0), (python_target_rect.x + 25, python_target_rect.y, 25, 50))\\n        python_label = font.render(\"Python\", True, (255, 255, 255))\\n        screen.blit(python_label, (python_target_rect.centerx - python_label.get_width() // 2,\\n                                   python_target_rect.top - python_label.get_height() - 5))\\n\\n        # Render Tool button.\\n        tool_target_rect = pygame.Rect(*tool_target_pos, 50, 50)\\n        if tool_image:\\n            screen.blit(tool_image, tool_target_rect.topleft)\\n        else:  # Fallback rendering\\n            pygame.draw.rect(screen, (128, 128, 128), tool_target_rect)\\n        tool_label = font.render(\"Tool\", True, (255, 255, 255))\\n        screen.blit(tool_label, (tool_target_rect.centerx - tool_label.get_width() // 2,\\n                                 tool_target_rect.top - tool_label.get_height() - 5))\\n\\n        # Trigger mode transitions only when agent arrives at the target\\n        if self.desired_position is None:  # Only trigger if not moving\\n            if (agent_x, agent_y) == python_target_pos:\\n                self.mode = \"python_code\"\\n                # Schedule Tkinter window display using the queue\\n                self._schedule_tk_update(self.show_python_code_window)\\n            elif (agent_x, agent_y) == tool_target_pos:\\n                self.mode = \"tool_selection\"\\n                self.selected_tool = None\\n                self.tool_info_shown = False\\n\\n    def _render_tool_selection(self, screen, font, agent_x, agent_y):\\n        header = font.render(\"Tool Selection Mode\", True, (255, 255, 255))\\n        screen.blit(header, (WINDOW_SIZE[0] //\\n                    2 - header.get_width() // 2, 20))\\n\\n        tools = list(tool_registry.keys())\\n        total = len(tools)\\n        if total == 0:\\n            no_tools_label = font.render(\\n                \"No tools registered\", True, (255, 100, 100))\\n            screen.blit(\\n                no_tools_label, (WINDOW_SIZE[0] // 2 - no_tools_label.get_width() // 2, 100))\\n            return  # Nothing else to render\\n\\n        for i, tool_name in enumerate(tools):\\n            pos = self.get_tool_position(i, total)\\n            # Center the rect on pos\\n            rect = pygame.Rect(pos[0] - 25, pos[1] - 25, 50, 50)\\n            color = (100 + i * (155 // total) if total >\\n                     0 else 100, 100, 150)  # Adjust color spread\\n            pygame.draw.rect(screen, color, rect)\\n            label = font.render(tool_name, True, (255, 255, 255))\\n            screen.blit(\\n                label, (rect.centerx - label.get_width() // 2, rect.top - label.get_height() - 5))\\n\\n            # Check for arrival at a tool position\\n            if self.desired_position is None and (agent_x, agent_y) == pos:\\n                if self.selected_tool == tool_name and not self.tool_info_shown:\\n                    self.tool_info_shown = True\\n                    # Schedule Tkinter window using the queue\\n                    self._schedule_tk_update(\\n                        self.show_tool_info_window, self.selected_tool)\\n                elif self.selected_tool != tool_name:\\n                    # This case shouldn\\'t happen if move_to_tool_dynamic sets selected_tool correctly\\n                    print(\\n                        f\"Warning: Agent arrived at tool \\'{tool_name}\\' but expected \\'{self.selected_tool}\\'\")\\n\\n    def _render_python_code(self, screen, font):\\n        header = font.render(\"Python Code Mode\", True, (255, 255, 255))\\n        screen.blit(header, (WINDOW_SIZE[0] //\\n                    2 - header.get_width() // 2, 20))\\n        if self.generated_python_code:\\n            lines = self.generated_python_code.splitlines()\\n            y_offset = 60\\n            max_lines = (WINDOW_SIZE[1] - y_offset - 50) // font.get_linesize()  # Limit lines shown\\n            for i, line in enumerate(lines):\\n                if i >= max_lines:\\n                    more_lines_surf = font.render(\"...\", True, (200, 200, 200))\\n                    screen.blit(more_lines_surf, (50, y_offset))\\n                    break\\n                code_surface = font.render(line, True, (200, 200, 200))\\n                screen.blit(code_surface, (50, y_offset))\\n                y_offset += font.get_linesize()\\n        else:\\n            prompt = font.render(\\n                \"Executing default code...\", True, (255, 255, 255))\\n            screen.blit(\\n                prompt, (WINDOW_SIZE[0] // 2 - prompt.get_width() // 2, 60))\\n\\n    def _draw_agent(self, screen, font, agent_x, agent_y):\\n        # Center the agent rect on its position\\n        agent_rect = pygame.Rect(agent_x - 25, agent_y - 25, 50, 50)\\n        pygame.draw.rect(screen, (255, 0, 0), agent_rect)\\n        agent_label = font.render(\"Agent\", True, (255, 255, 255))\\n        screen.blit(agent_label, (agent_rect.centerx - agent_label.get_width() // 2,\\n                                  agent_rect.top - agent_label.get_height() - 5))\\n\\n    def _close_code_window_and_return(self, window: tk.Toplevel):\\n        \"\"\"Helper to close the code window and trigger return.\"\"\"\\n        try:\\n            window.destroy()\\n        except tk.TclError:\\n            pass  # Window might already be closed\\n        self.return_to_normal()\\n\\n    def show_python_code_window(self) -> None:\\n        self._ensure_tk_root()\\n        code_window = tk.Toplevel(self.tk_root)\\n        code_window.title(\"Python Code Execution\")\\n        code_window.transient(self.tk_root)  # Associate with main window\\n        code_window.grab_set()  # Make modal\\n\\n        code_to_run = self.generated_python_code if self.generated_python_code else DEFAULT_CODE\\n        code_title = \"Generated Python Code\" if self.generated_python_code else \"Default Python Code\"\\n\\n        title_label = tk.Label(\\n            code_window, text=code_title, font=(\"Arial\", 12, \"bold\"))\\n        title_label.pack(padx=10, pady=(10, 5))\\n\\n        code_text = tk.Text(code_window, height=10, width=60)\\n        code_text.pack(padx=10, pady=5)\\n        code_text.insert(tk.END, code_to_run)\\n        code_text.config(state=tk.DISABLED)\\n\\n        output_label = tk.Label(code_window, text=\"Output:\")\\n        output_label.pack(padx=10, pady=(10, 0))\\n        output_text = tk.Text(code_window, height=5, width=60)\\n        output_text.pack(padx=10, pady=5)\\n        output_text.insert(tk.END, \"Click \\'Execute\\' to run...\")\\n        output_text.config(state=tk.DISABLED)\\n\\n        # Use a variable to prevent multiple executions\\n        execution_done = threading.Event()\\n\\n        def execute_code_action() -> None:\\n            if execution_done.is_set():\\n                return  # Already executed or executing\\n\\n            execution_done.set()\\n            execute_button.config(state=tk.DISABLED)  # Disable button during execution\\n            output_text.config(state=tk.NORMAL)\\n            output_text.delete(\"1.0\", tk.END)\\n            output_text.insert(tk.END, \"Executing...\")\\n            output_text.config(state=tk.DISABLED)\\n            code_window.update_idletasks()  # Ensure \"Executing...\" is shown\\n\\n            f = io.StringIO()\\n            try:\\n                with contextlib.redirect_stdout(f):\\n                    # Note: exec is generally unsafe if code comes from untrusted sources.\\n                    # Here it\\'s assumed the generated code is controlled/reviewed.\\n                    exec(code_to_run)\\n                output = f.getvalue()\\n                if not output:\\n                    output = \"[No output produced]\"\\n            except Exception as e:\\n                output = f\"Error during execution:\\\\n{e}\\\\n{traceback.format_exc()}\"\\n            finally:\\n                f.close()\\n\\n            output_text.config(state=tk.NORMAL)\\n            output_text.delete(\"1.0\", tk.END)\\n            output_text.insert(tk.END, output)\\n            output_text.config(state=tk.DISABLED)\\n\\n            # Schedule window closure and return to normal mode\\n            code_window.after(\\n                3000, self._close_code_window_and_return, code_window)\\n\\n        execute_button = tk.Button(\\n            code_window, text=\"Execute Code\", command=execute_code_action)\\n        execute_button.pack(padx=10, pady=10)\\n\\n        # Automatically start execution\\n        code_window.after(100, execute_code_action)\\n\\n    def show_tool_info_window(self, tool_name: str) -> None:\\n        self._ensure_tk_root()\\n        info_window = tk.Toplevel(self.tk_root)\\n        info_window.title(f\"Tool Information - {tool_name}\")\\n        info_window.transient(self.tk_root)  # Associate with main window\\n        info_window.grab_set()  # Make modal\\n\\n        info_text = tk.Text(info_window, height=10, width=60, wrap=tk.WORD)\\n        info_text.pack(padx=10, pady=10)\\n\\n        tool_func = tool_registry.get(tool_name)\\n        doc = \"Documentation not available.\"\\n        if tool_func:\\n            doc = tool_func.__doc__ if tool_func.__doc__ else \"No documentation provided.\"\\n\\n        info_text.insert(tk.END, doc.strip())\\n        info_text.config(state=tk.DISABLED)\\n\\n        def close_and_return():\\n            try:\\n                info_window.destroy()\\n            except tk.TclError:\\n                pass  # Window might already be closed\\n            # Optionally, return agent to start immediately after closing tool info\\n            # self.return_to_normal() # Or handle this transition elsewhere\\n\\n        close_button = tk.Button(\\n            info_window, text=\"Close\", command=close_and_return)\\n        close_button.pack(padx=10, pady=10)\\n\\n        # Automatically close after a delay and return agent to start\\n        info_window.after(4000, close_and_return)\\n        info_window.after(4100, self.return_to_normal)  # Schedule return slightly after close\\n\\n    # Navigation methods.\\n    def move_to_python(self) -> None:\\n        if self.mode == \"normal\":\\n            self.desired_position = (50, 100)  # Target position for Python\\n            print(\"Agent moving to Python interaction point.\")\\n\\n    def move_to_tool(self) -> None:\\n        if self.mode == \"normal\":\\n            self.desired_position = (500, 100)  # Target position for Tool Selection entry\\n            print(\"Agent moving to Tool interaction point.\")\\n\\n    def return_to_start(self) -> None:\\n        # This function might be redundant if return_to_normal always goes to start\\n        if self.mode == \"normal\":\\n            self.desired_position = (300, 400)  # Center start position\\n            print(\"Agent returning to start position.\")\\n\\n    def move_to_tool_dynamic(self, tool_name: str) -> None:\\n        if self.mode == \"tool_selection\":\\n            try:\\n                tools = list(tool_registry.keys())\\n                if tool_name in tools:\\n                    index = tools.index(tool_name)\\n                    pos = self.get_tool_position(index, len(tools))\\n                    self.desired_position = pos  # Target position for the specific tool\\n                    self.selected_tool = tool_name\\n                    self.tool_info_shown = False  # Reset flag for showing info window\\n                    print(f\"Agent moving to tool: {tool_name}\")\\n                else:\\n                    print(\\n                        f\"Warning: Tool \\'{tool_name}\\' not found in registry for movement.\")\\n            except ValueError:\\n                print(f\"Error: Tool \\'{tool_name}\\' not found while trying to move.\")\\n        else:\\n            print(f\"Warning: Cannot move to tool \\'{tool_name}\\' from mode \\'{self.mode}\\'.\")\\n\\n    def return_to_normal(self) -> None:\\n        \"\"\"Sets mode to normal and moves agent towards the start position.\"\"\"\\n        if self.mode in (\"tool_selection\", \"python_code\"):\\n            print(f\"Returning to normal mode from {self.mode}.\")\\n            self.mode = \"normal\"\\n            self.selected_tool = None\\n            self.tool_info_shown = False\\n            self.generated_python_code = \"\"  # Clear generated code display\\n            self.desired_position = (300, 400)  # Center start position\\n        elif self.mode == \"normal\" and self.desired_position is None:\\n            # If already normal and not moving, ensure agent is at start\\n            self.desired_position = (300, 400)\\n\\n    def launch_agent_interaction(self) -> None:\\n        \"\"\"Public method to launch the game, ensuring Tk root is available.\"\"\"\\n        self._ensure_tk_root()  # Make sure Tkinter is initialized\\n        self.launch_game()\\n\\n\\n# When running this module directly.\\nif __name__ == \"__main__\":\\n    root = tk.Tk()\\n    root.title(\"Agent Interaction Test Controller\")\\n    manager = AgentInteractionManager()\\n    manager.tk_root = root  # Use the main Tkinter window for this test setup\\n    # manager._process_tk_queue() # Start the queue processor\\n\\n    # Keep a reference to the button to re-enable it\\n    manager.launch_button = tk.Button(\\n        root, text=\"Launch Agent Simulation\", command=manager.launch_agent_interaction)\\n    manager.launch_button.pack(padx=20, pady=20)\\n\\n    control_frame = tk.Frame(root)\\n    control_frame.pack(padx=20, pady=10)\\n\\n    tk.Label(control_frame, text=\"Manual Agent Control:\").pack()\\n\\n    python_choice = tk.Button(\\n        control_frame, text=\"Move to Python\", command=manager.move_to_python)\\n    python_choice.pack(side=tk.LEFT, padx=5)\\n\\n    tool_choice = tk.Button(control_frame, text=\"Move to Tool Select\",\\n                            command=manager.move_to_tool)\\n    tool_choice.pack(side=tk.LEFT, padx=5)\\n\\n    # Example dynamic tool move buttons\\n    tool_names = list(tool_registry.keys())\\n    if tool_names:\\n        # Only show if tools exist\\n        dynamic_tool_frame = tk.Frame(root)\\n        dynamic_tool_frame.pack(padx=20, pady=10)\\n        tk.Label(dynamic_tool_frame,\\n                 text=\"Move to Specific Tool (requires Tool Select mode):\").pack()\\n        for name in tool_names[:3]:  # Show first few tools as example\\n            btn = tk.Button(dynamic_tool_frame, text=f\"Move to {name}\",\\n                             command=lambda n=name: manager.move_to_tool_dynamic(n))\\n            btn.pack(side=tk.LEFT, padx=5)\\n\\n    return_normal_button = tk.Button(\\n        control_frame, text=\"Return to Normal/Start\", command=manager.return_to_normal)\\n    return_normal_button.pack(side=tk.LEFT, padx=5)\\n\\n    def on_closing():\\n        print(\"Controller window closing...\")\\n        manager.stop_game()  # Signal pygame to stop if running\\n        root.destroy()\\n\\n    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\\n    root.mainloop()\\n'},\n",
              " {'simple\\\\Guide.md': '\\n**I. Core Agent Logic & LLM Interaction**\\n\\n1.  **Smarter Error Handling & Retries:**\\n    *   Instead of just retrying on JSON errors, analyze the *type* of error (e.g., missing key, invalid format) and provide more specific correction hints to the LLM in `_build_correction_prompt`.\\n    *   Implement exponential backoff for retries, especially if interacting with external APIs that might be temporarily unavailable.\\n    *   Consider a mechanism where if an action (python/tool) fails repeatedly, the agent tries a *different* action type instead of getting stuck.\\n\\n2.  **Context Window Management:**\\n    *   Explicitly manage the context window size passed to the LLM. Long histories, large code contexts, and detailed logs can exceed limits.\\n    *   Implement token counting and truncation strategies (e.g., keep first/last N tokens, summarize older parts more aggressively). The current summarization approach is good, but ensure it fits within the context limit along with the current request and action log.\\n\\n3.  **Planning Capabilities:**\\n    *   Instead of a purely reactive Decide->Act->Check loop, consider adding a planning step where the LLM outlines multiple steps *before* starting execution. This can lead to more coherent and efficient task completion for complex requests. The agent could then execute steps sequentially, checking progress against the plan.\\n\\n4.  **Refined Prompt Engineering:**\\n    *   Continuously refine the system prompts (`system_prompts.py`) based on observed failure modes. Make instructions for JSON formatting even more explicit.\\n    *   Experiment with different prompt structures (e.g., few-shot examples within the prompt for complex JSON).\\n    *   Ensure the `summary` context is concise and truly helpful, not just adding noise.\\n\\n5.  **Model Abstraction:**\\n    *   While `inference.py` uses Ollama, abstract the client interaction further. Create a base `LLMClient` class and specific implementations (e.g., `OllamaClient`, `OpenAIClient`). This would allow easier switching or adding support for different model providers/APIs.\\n\\n**II. Tools & Code Execution**\\n\\n1.  **Asynchronous Tools:**\\n    *   If tools involve I/O operations (network requests, long file operations), make them `async` functions and integrate them properly with the asyncio event loop used in `agent_executor.py`.\\n\\n**III. GUI & User Experience**\\n\\n1.  **Responsiveness & Threading:**\\n    *   Ensure *all* potentially long-running operations (LLM calls, code execution, tool execution, file I/O, Pygame rendering) happen off the main Tkinter thread to prevent the GUI from freezing. The current use of `run_async_in_thread` is good; verify its consistent application.\\n    *   Provide clearer visual feedback during long operations (e.g., progress bars, more specific status messages than just \"Loading...\"). The Pygame visualization helps here but could be augmented in the Tkinter UI too.\\n\\n2.  **Clarity of Output Areas:**\\n    *   Consider reorganizing the `Interaction Output`, `Action Output`, and `Scratchpad` areas. Maybe tab views with filtering options? Ensure labels clearly define what each area shows.\\n    *   Use distinct formatting (colors, fonts, spacing) within the text areas to differentiate between user input, LLM thoughts, tool calls, code blocks, errors, and final answers.\\n\\n3.  **Pygame Visualization (`agent_interactions.py`):**\\n    *   **State:** Show more agent state visually (e.g., current \\'thought\\' or action being performed).\\n    *   **Integration:** Make the simulation window less intrusive. Could it be embedded within the Tkinter app as a panel instead of a separate Pygame window?\\n    *   **Error Handling:** Improve handling of Pygame errors (e.g., missing image assets) so they don\\'t crash the simulation thread unexpectedly. Use fallback rendering more consistently (as done for Python/Tool icons).\\n    *   **Queue Safety:** The use of `queue.Queue` and `_schedule_tk_update` for cross-thread Tkinter updates is good practice. Ensure all Tkinter modifications from the Pygame thread go through this mechanism.\\n\\n4.  **Configuration Management:**\\n    *   Allow saving/loading GUI configurations (model selection, codebase path, tips).\\n    *   Dynamically populate the model dropdown by querying the Ollama API (`ollama list`).\\n    *   Add a \"Browse...\" button for the codebase directory path.\\n\\n5.  **Input History:**\\n    *   Add search functionality to the input history.\\n    *   Allow deleting specific history entries.\\n\\n6.  **Auto-Prompt Feature:**\\n    *   Make the auto-prompt (`check_input`, `auto_fill_and_submit_prompt`) optional via a checkbox in the GUI, as it can be intrusive.\\n    *   Ensure the auto-prompt generation itself doesn\\'t block the GUI.\\n\\n**IV. Memory & History**\\n\\n1.  **Vector DB Integration:**\\n    *   Ensure the embedding retrieval (`retrieve_embeddings` in `memory.py`) is actively used in the main agent loop (`agent_executor.py`) to augment prompts when relevant context might exist in memory. It\\'s currently shown as example usage but not integrated into `agent_execution`.\\n    *   Provide UI feedback when memory is being queried or added to.\\n    *   Allow the user to view/clear the vector database contents via the GUI.\\n\\n2.  **Efficiency:**\\n    *   Embedding generation and similarity search can be slow. Consider optimizing queries or running these tasks asynchronously with feedback.\\n\\n3.  **Hybrid History:**\\n    *   Combine the simple JSON history (`history.py`) with the vector memory. Use the JSON for sequential conversation flow and the vector DB for semantic searching of past interactions/knowledge.\\n\\n**V. Code Quality & Maintainability**\\n\\n1.  **Dependency Management:**\\n    *   Create a `requirements.txt` or use `pyproject.toml` to list all dependencies (Tkinter, Pygame, Ollama, Matplotlib, Diffusers, Torch, ChromaDB, Requests, etc.).\\n    *   Remove duplicated code like `flatted.py`. If it\\'s a necessary dependency, include it once or add it to requirements.\\n\\n2.  **Testing:**\\n    *   Implement unit tests for core components (e.g., tool functions, utility functions, prompt generation).\\n    *   Implement integration tests for the agent execution loop (mocking the LLM).\\n\\n3.  **Documentation:**\\n    *   Add more detailed docstrings, especially for complex classes like `AgentExecutor` and `AgentInteractionManager`.\\n    *   Create a project README explaining setup, usage, architecture, and security considerations.\\n\\n4.  **Configuration Files:**\\n    *   Move hardcoded settings (like `AGENT_SPEED`, `WINDOW_SIZE`, `MAX_RETRIES`, model names, history paths) into a configuration file (e.g., YAML, TOML, JSON) or environment variables.\\n'},\n",
              " {'simple\\\\GuideBase.md': '\\n**I. Core Agent Logic & LLM Interaction**\\n\\n1.  **Smarter Error Handling & Retries:**\\n    *   Instead of just retrying on JSON errors, analyze the *type* of error (e.g., missing key, invalid format) and provide more specific correction hints to the LLM in `_build_correction_prompt`.\\n    *   Implement exponential backoff for retries, especially if interacting with external APIs that might be temporarily unavailable.\\n    *   Consider a mechanism where if an action (python/tool) fails repeatedly, the agent tries a *different* action type instead of getting stuck.\\n\\n2.  **Context Window Management:**\\n    *   Explicitly manage the context window size passed to the LLM. Long histories, large code contexts, and detailed logs can exceed limits.\\n    *   Implement token counting and truncation strategies (e.g., keep first/last N tokens, summarize older parts more aggressively). The current summarization approach is good, but ensure it fits within the context limit along with the current request and action log.\\n\\n3.  **Planning Capabilities:**\\n    *   Instead of a purely reactive Decide->Act->Check loop, consider adding a planning step where the LLM outlines multiple steps *before* starting execution. This can lead to more coherent and efficient task completion for complex requests. The agent could then execute steps sequentially, checking progress against the plan.\\n\\n4.  **Refined Prompt Engineering:**\\n    *   Continuously refine the system prompts (`system_prompts.py`) based on observed failure modes. Make instructions for JSON formatting even more explicit.\\n    *   Experiment with different prompt structures (e.g., few-shot examples within the prompt for complex JSON).\\n    *   Ensure the `summary` context is concise and truly helpful, not just adding noise.\\n\\n5.  **Model Abstraction:**\\n    *   While `inference.py` uses Ollama, abstract the client interaction further. Create a base `LLMClient` class and specific implementations (e.g., `OllamaClient`, `OpenAIClient`). This would allow easier switching or adding support for different model providers/APIs.\\n\\n**II. Tools & Code Execution**\\n\\n1.  **SECURITY - Sandboxing:**\\n    *   **CRITICAL:** `execute_bash_command` in `function_call.py` is extremely dangerous. Avoid executing arbitrary shell commands generated by the LLM. Replace it with specific, safer Python functions for intended tasks (e.g., a function to list specific directory contents, a function to run `git status`, etc.) or implement strict command whitelisting/sanitization.\\n    *   **CRITICAL:** `exec()` in `agent_interactions.py` (within `show_python_code_window`) should ideally be replaced with the safer `subprocess` method used in `code_execute.py` if the goal is just to *show* execution results. If dynamic execution within the main process is needed, it requires extreme caution and potentially sandboxing libraries (like `RestrictedPython`), though `subprocess` is generally preferred for isolation.\\n    *   Consider running `execute_python_code` within a Docker container or using OS-level sandboxing (like `firejail` on Linux) for better isolation, preventing accidental or malicious filesystem access/network calls beyond what\\'s intended.\\n\\n2.  **Tool Input/Output Validation:**\\n    *   Add input validation (using type hints, Pydantic, or manual checks) within the tool functions themselves (`custom/*.py`, `function_call.py`).\\n    *   Define expected output formats for tools, potentially returning structured data (JSON) instead of just strings, making it easier for the LLM to parse results.\\n\\n3.  **Asynchronous Tools:**\\n    *   If tools involve I/O operations (network requests, long file operations), make them `async` functions and integrate them properly with the asyncio event loop used in `agent_executor.py`.\\n\\n4.  **Tool Management GUI:**\\n    *   Add a section in the Tkinter GUI to list available tools (from `tool_registry`), view their documentation, and potentially enable/disable them.\\n\\n5.  **Dependency Management for Tools:**\\n    *   If custom tools require external libraries, implement a mechanism to declare and potentially install these dependencies (e.g., reading a `requirements.txt` associated with the tool).\\n\\n**III. GUI & User Experience**\\n\\n1.  **Responsiveness & Threading:**\\n    *   Ensure *all* potentially long-running operations (LLM calls, code execution, tool execution, file I/O, Pygame rendering) happen off the main Tkinter thread to prevent the GUI from freezing. The current use of `run_async_in_thread` is good; verify its consistent application.\\n    *   Provide clearer visual feedback during long operations (e.g., progress bars, more specific status messages than just \"Loading...\"). The Pygame visualization helps here but could be augmented in the Tkinter UI too.\\n\\n2.  **Clarity of Output Areas:**\\n    *   Consider combining or reorganizing the `Interaction Output`, `Action Output`, and `Scratchpad` areas. Maybe tabs or a single log view with filtering options? Ensure labels clearly define what each area shows.\\n    *   Use distinct formatting (colors, fonts, spacing) within the text areas to differentiate between user input, LLM thoughts, tool calls, code blocks, errors, and final answers.\\n\\n3.  **Pygame Visualization (`agent_interactions.py`):**\\n    *   **State:** Show more agent state visually (e.g., current \\'thought\\' or action being performed).\\n    *   **Integration:** Make the simulation window less intrusive. Could it be embedded within the Tkinter app as a panel instead of a separate Pygame window?\\n    *   **Error Handling:** Improve handling of Pygame errors (e.g., missing image assets) so they don\\'t crash the simulation thread unexpectedly. Use fallback rendering more consistently (as done for Python/Tool icons).\\n    *   **Queue Safety:** The use of `queue.Queue` and `_schedule_tk_update` for cross-thread Tkinter updates is good practice. Ensure all Tkinter modifications from the Pygame thread go through this mechanism.\\n\\n4.  **Configuration Management:**\\n    *   Allow saving/loading GUI configurations (model selection, codebase path, tips).\\n    *   Dynamically populate the model dropdown by querying the Ollama API (`ollama list`).\\n    *   Add a \"Browse...\" button for the codebase directory path.\\n\\n5.  **Input History:**\\n    *   Add search functionality to the input history.\\n    *   Allow deleting specific history entries.\\n\\n6.  **Auto-Prompt Feature:**\\n    *   Make the auto-prompt (`check_input`, `auto_fill_and_submit_prompt`) optional via a checkbox in the GUI, as it can be intrusive.\\n    *   Ensure the auto-prompt generation itself doesn\\'t block the GUI.\\n\\n7.  **Image Generation Integration:**\\n    *   Clarify how `imgen.py` and `imregen.py` are intended to be used. Are they tools? Add them to the `tool_registry` if so.\\n    *   Provide UI elements (e.g., a button, specific command) to trigger image generation and display the results within the GUI. `imregen.py` seems unfinished.\\n\\n**IV. Memory & History**\\n\\n1.  **Vector DB Integration:**\\n    *   Ensure the embedding retrieval (`retrieve_embeddings` in `memory.py`) is actively used in the main agent loop (`agent_executor.py`) to augment prompts when relevant context might exist in memory. It\\'s currently shown as example usage but not integrated into `agent_execution`.\\n    *   Provide UI feedback when memory is being queried or added to.\\n    *   Allow the user to view/clear the vector database contents via the GUI.\\n\\n2.  **Efficiency:**\\n    *   Embedding generation and similarity search can be slow. Consider optimizing queries or running these tasks asynchronously with feedback.\\n\\n3.  **Hybrid History:**\\n    *   Combine the simple JSON history (`history.py`) with the vector memory. Use the JSON for sequential conversation flow and the vector DB for semantic searching of past interactions/knowledge.\\n\\n**V. Code Quality & Maintainability**\\n\\n1.  **Dependency Management:**\\n    *   Create a `requirements.txt` or use `pyproject.toml` to list all dependencies (Tkinter, Pygame, Ollama, Matplotlib, Diffusers, Torch, ChromaDB, Requests, etc.).\\n    *   Remove duplicated code like `flatted.py`. If it\\'s a necessary dependency, include it once or add it to requirements.\\n\\n2.  **Testing:**\\n    *   Implement unit tests for core components (e.g., tool functions, utility functions, prompt generation).\\n    *   Implement integration tests for the agent execution loop (mocking the LLM).\\n\\n3.  **Documentation:**\\n    *   Add more detailed docstrings, especially for complex classes like `AgentExecutor` and `AgentInteractionManager`.\\n    *   Create a project README explaining setup, usage, architecture, and security considerations.\\n\\n4.  **Configuration Files:**\\n    *   Move hardcoded settings (like `AGENT_SPEED`, `WINDOW_SIZE`, `MAX_RETRIES`, model names, history paths) into a configuration file (e.g., YAML, TOML, JSON) or environment variables.\\n'},\n",
              " {'simple\\\\makeArt.py': '#!/usr/bin/env python3\\n\"\"\"\\nCreates an artistic PNG image from data using matplotlib.\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt # type: ignore\\nimport numpy as np # type: ignore\\nimport os\\nimport logging\\nfrom typing import List\\nfrom simple.code.utils import colored_print, strip_model_escapes, Fore\\nfrom simple.code.logging_config import setup_logging\\n\\nsetup_logging()\\n\\n\\ndef create_artistic_png(data: List[float], filename: str = \"simple/gag/artistic_plot.png\", style: str = \"seaborn-darkgrid\") -> None:\\n    \"\"\"\\n    Creates an artistic PNG image from the provided data using a specified style.\\n\\n    Enhancements:\\n    - Uses a custom style if available.\\n    - Colors data points using a colormap.\\n    - Combines scatter and line plots.\\n    - Adds annotations for each data point.\\n\\n    Args:\\n        data (List[float]): List of numeric values to plot.\\n        filename (str, optional): Output file path for the PNG image.\\n        style (str, optional): Matplotlib style to use.\\n    \"\"\"\\n    output_dir = os.path.dirname(filename)\\n    if output_dir and not os.path.exists(output_dir):\\n        os.makedirs(output_dir, exist_ok=True)\\n        logging.info(f\"Created directory {output_dir} for output image.\")\\n\\n    available_styles = plt.style.available\\n    if style in available_styles:\\n        plt.style.use(style)\\n    else:\\n        logging.warning(\\n            f\"Style \\'{style}\\' is not available. Using default style instead.\")\\n\\n    fig, ax = plt.subplots(figsize=(10, 8))\\n    x = np.arange(len(data))\\n    scatter = ax.scatter(x, data, c=data, cmap=\\'viridis\\',\\n                         s=120, alpha=0.8, edgecolor=\\'black\\')\\n    ax.plot(x, data, color=\\'white\\', linewidth=2, linestyle=\\'--\\')\\n    cbar = plt.colorbar(scatter, ax=ax)\\n    cbar.set_label(\\'Data Value\\', rotation=270, labelpad=15)\\n    ax.set_title(\"Artistic Data Plot\", fontsize=20,\\n                 fontweight=\\'bold\\', color=\\'navy\\')\\n    ax.set_xlabel(\"Index\", fontsize=14)\\n    ax.set_ylabel(\"Value\", fontsize=14)\\n\\n    for i, value in enumerate(data):\\n        ax.annotate(f\\'{value}\\', xy=(i, value), xytext=(5, 5), textcoords=\\'offset points\\',\\n                    fontsize=10, color=\\'darkred\\')\\n\\n    try:\\n        plt.savefig(filename, format=\"png\", dpi=300, bbox_inches=\\'tight\\')\\n        logging.info(f\"Artistic PNG image saved as \\'{filename}\\'.\")\\n    except Exception as e:\\n        logging.error(f\"Failed to save image {filename}: {e}\")\\n    finally:\\n        plt.close()\\n\\n\\nif __name__ == \"__main__\":\\n    example_data = [1, 3, 2, 5, 7, 8, 6]\\n    create_artistic_png(example_data)\\n    print(\"Artistic PNG image created as \\'simple/gag/artistic_plot.png\\'.\")\\n'},\n",
              " {'simple\\\\app\\\\base\\\\next-env.d.ts': '/// <reference types=\"next\" />\\n/// <reference types=\"next/image-types/global\" />\\n/// <reference types=\"next/navigation-types/compat/navigation\" />\\n\\n// NOTE: This file should not be edited\\n// see https://nextjs.org/docs/app/api-reference/config/typescript for more information.\\n'},\n",
              " {'simple\\\\app\\\\base\\\\next.config.ts': 'import type { NextConfig } from \"next\";\\n\\nconst nextConfig: NextConfig = {\\n  /* config options here */\\n};\\n\\nexport default nextConfig;\\n'},\n",
              " {'simple\\\\app\\\\base\\\\README.md': '## Getting Started\\n\\nFirst, run the development server:\\n\\n```bash\\nnpm run dev\\n\\n```\\n\\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\\n\\nYou can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.\\n\\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\\n'},\n",
              " {'simple\\\\app\\\\base\\\\src\\\\app\\\\layout.tsx': 'import type { Metadata } from \"next\";\\nimport { Geist, Geist_Mono } from \"next/font/google\";\\nimport \"./globals.css\";\\n\\nconst geistSans = Geist({\\n  variable: \"--font-geist-sans\",\\n  subsets: [\"latin\"],\\n});\\n\\nconst geistMono = Geist_Mono({\\n  variable: \"--font-geist-mono\",\\n  subsets: [\"latin\"],\\n});\\n\\nexport const metadata: Metadata = {\\n  title: \"Create Next App\",\\n  description: \"Generated by create next app\",\\n};\\n\\nexport default function RootLayout({\\n  children,\\n}: Readonly<{\\n  children: React.ReactNode;\\n}>) {\\n  return (\\n    <html lang=\"en\">\\n      <body\\n        className={`${geistSans.variable} ${geistMono.variable} antialiased`}\\n      >\\n        {children}\\n      </body>\\n    </html>\\n  );\\n}\\n'},\n",
              " {'simple\\\\app\\\\base\\\\src\\\\app\\\\page.tsx': '// pages/index.tsx\\n\"use client\"\\nimport { useState } from \\'react\\';\\n\\nexport default function Home() {\\n  const [userInput, setUserInput] = useState(\\'\\');\\n  const [conversation, setConversation] = useState<Array<{ role: string, content: string }>>([]);\\n\\n  const handleChatSubmit = async (e: React.FormEvent) => {\\n    e.preventDefault();\\n    if (!userInput.trim()) return;\\n\\n    // Append the user message to the conversation for chat\\n    setConversation(prev => [...prev, { role: \\'user\\', content: userInput }]);\\n\\n    try {\\n      const res = await fetch(\\'/api/chat\\', {\\n        method: \\'POST\\',\\n        headers: { \\'Content-Type\\': \\'application/json\\' },\\n        body: JSON.stringify({ userInput })\\n      });\\n      const data = await res.json();\\n      setConversation(prev => [...prev, { role: \\'assistant\\', content: data.response }]);\\n      setUserInput(\\'\\');\\n    } catch (err) {\\n      console.error(err);\\n    }\\n  };\\n\\n  const handleGenerate = async () => {\\n    if (!userInput.trim()) return;\\n\\n    // Append the user message to the conversation for generate\\n    setConversation(prev => [...prev, { role: \\'user\\', content: userInput }]);\\n\\n    try {\\n      const res = await fetch(\\'/api/generate\\', {\\n        method: \\'POST\\',\\n        headers: { \\'Content-Type\\': \\'application/json\\' },\\n        body: JSON.stringify({ userInput })\\n      });\\n      const data = await res.json();\\n      setConversation(prev => [...prev, { role: \\'assistant\\', content: data.response }]);\\n      setUserInput(\\'\\');\\n    } catch (err) {\\n      console.error(err);\\n    }\\n  };\\n\\n  return (\\n    <div style={{ margin: \\'2rem\\' }}>\\n      <h1>Chat with Ollama</h1>\\n      <div style={{ marginBottom: \\'1rem\\', border: \\'1px solid #ccc\\', padding: \\'1rem\\', minHeight: \\'300px\\' }}>\\n        {conversation.map((msg, idx) => (\\n          <p key={idx}><strong>{msg.role}:</strong> {msg.content}</p>\\n        ))}\\n      </div>\\n      <form onSubmit={handleChatSubmit}>\\n        <input\\n          type=\"text\"\\n          value={userInput}\\n          onChange={(e) => setUserInput(e.target.value)}\\n          placeholder=\"Type your message\"\\n          style={{ width: \\'80%\\', marginRight: \\'1rem\\' }}\\n        />\\n        <button type=\"submit\">Chat</button>\\n        <button type=\"button\" onClick={handleGenerate} style={{ marginLeft: \\'0.5rem\\' }}>\\n          Generate\\n        </button>\\n      </form>\\n    </div>\\n  );\\n}\\n'},\n",
              " {'simple\\\\app\\\\base\\\\src\\\\pages\\\\api\\\\chat.ts': 'import type { NextApiRequest, NextApiResponse } from \\'next\\';\\nimport { Ollama } from \\'ollama\\';\\n\\nconst ollama = new Ollama();\\n\\n// In-memory conversation history (demo purposes)\\nconst messages: Array<{ role: string, content: string }> = [];\\n\\nexport default async function handler(req: NextApiRequest, res: NextApiResponse) {\\n  if (req.method !== \\'POST\\') {\\n    res.setHeader(\\'Allow\\', [\\'POST\\']);\\n    return res.status(405).end(`Method ${req.method} Not Allowed`);\\n  }\\n  \\n  const { userInput } = req.body;\\n  if (!userInput) {\\n    return res.status(400).json({ error: \\'Missing userInput\\' });\\n  }\\n\\n  console.log(\"Ollama Chat: Received userInput:\", userInput);\\n  \\n  let responseContent = \"\";\\n  \\n  try {\\n    const chatMessages = [\\n      ...messages,\\n      { role: \"system\", content: \"You are a helpful assistant. You only give a short sentence by answer.\" },\\n      { role: \"user\", content: userInput }\\n    ];\\n    console.log(\"Ollama Chat: Calling ollama.chat with parameters:\", {\\n      model: \"llama3.2\",\\n      messages: chatMessages,\\n      stream: true,\\n    });\\n    \\n    // Call Ollama with streaming enabled\\n    const stream = await ollama.chat({\\n      model: \"llama3.2\",\\n      messages: chatMessages,\\n      stream: true,\\n    });\\n    \\n    for await (const chunk of stream) {\\n      if (chunk.message) {\\n        responseContent += chunk.message.content;\\n      }\\n    }\\n    \\n    console.log(\"Ollama Chat: Received responseContent:\", responseContent);\\n    \\n    // Update conversation history\\n    messages.push(\\n      { role: \"user\", content: userInput },\\n      { role: \"assistant\", content: responseContent }\\n    );\\n    \\n    res.status(200).json({ response: responseContent });\\n  } catch (error) {\\n    console.error(\"Ollama Chat: Error calling ollama.chat:\", error);\\n    res.status(500).json({ error: \\'Internal Server Error\\' });\\n  }\\n}\\n'},\n",
              " {'simple\\\\app\\\\base\\\\src\\\\pages\\\\api\\\\generate.ts': 'import type { NextApiRequest, NextApiResponse } from \\'next\\';\\nimport { Ollama } from \\'ollama\\';\\n\\nconst ollama = new Ollama();\\n\\nexport default async function handler(req: NextApiRequest, res: NextApiResponse) {\\n  if (req.method !== \\'POST\\') {\\n    res.setHeader(\\'Allow\\', [\\'POST\\']);\\n    return res.status(405).end(`Method ${req.method} Not Allowed`);\\n  }\\n\\n  const { userInput } = req.body;\\n  if (!userInput) {\\n    return res.status(400).json({ error: \\'Missing userInput\\' });\\n  }\\n\\n  let responseContent = \"\";\\n\\n  try {\\n    console.log(\"Ollama Generate: Received prompt:\", userInput);\\n    console.log(\"Ollama Generate: Calling ollama.generate with parameters:\", {\\n      model: \"llama3.2\",\\n      prompt: userInput,\\n      stream: true,\\n    });\\n\\n    // Call Ollama\\'s generate method with streaming enabled\\n    const stream = await ollama.generate({\\n      model: \"llama3.2\",\\n      prompt: userInput,\\n      stream: true\\n    });\\n\\n    for await (const chunk of stream) {\\n      if (chunk.response) {\\n        responseContent += chunk.response;\\n      }\\n    }\\n\\n    console.log(\"Ollama Generate: Received responseContent:\", responseContent);\\n    res.status(200).json({ response: responseContent });\\n  } catch (error) {\\n    console.error(\"Ollama Generate: Error calling ollama.generate:\", error);\\n    res.status(500).json({ error: \\'Internal Server Error\\' });\\n  }\\n}\\n'},\n",
              " {'simple\\\\app\\\\base\\\\src\\\\pages\\\\api\\\\transcribe.ts': 'import { StreamTranscriber } from \"@transcribe/transcriber\";\\n\\n// create new instance\\nconst streamTranscriber = new StreamTranscriber({\\n    createModule, // create module function from emscripten wasm build\\n    model: \"/your/project/model.bin\", // can be path to model file, or File() object\\n    workerPath: \"/your/project\", // set path to directory of shout.wasm.worker.mjs\\n    audioWorkletsPath: \"/your/project/audio-worklets\", // set path to directory of vad.js, buffer.js\\n\\n    // called on new transcription\\n    onSegment: (segment) => {\\n        console.log(segment);\\n    },\\n});\\n\\n// init wasm transcriber worker\\nawait streamTranscriber.init();\\n\\n// start transcriber wasm wait for audio\\nawait streamTranscriber.start({\\n    lang: \"en\", // language code\\n    threads: this.maxThreads, // max threads to use\\n    translate: false, // translate to english\\n    suppress_non_speech: true, // ignore non speech tokens\\n    max_tokens: 32, // max token number\\n    audio_ctx: 756, // size of audio context\\n});\\n\\n// create audio context and media stream\\nconst mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });\\n\\n// start transcribing\\n// audio gets transcribed when silence is detected or maxRecordMs is reached\\nstreamTranscriber.transcribe(stream, {\\n    preRecordMs: 200, // pre record audio, because vad needs some time\\n    maxRecordMs: 5000, // max buffer size in milliseconds\\n    minSilenceMs: 500, // min time of silence before call transcribe\\n    onVoiceActivity: (active) => {\\n        console.log(\"Voice Activity: \", active);\\n    },\\n});\\n\\n// ...do stuff\\n\\n// stop stream when not needed\\nawait streamTranscriber.stop();\\n\\n// destroy instance\\nstreamTranscriber.destroy();'},\n",
              " {'simple\\\\code\\\\agent_executor.py': '# ./simple/code/agent_executor.py:\\nimport json\\nimport logging\\nimport re\\nfrom typing import Any, List, Optional, Tuple, Dict, Union  # Added Dict, Union\\n\\nfrom simple.agent_interactions import AgentInteractionManager\\nfrom simple.code.inference import run_inference\\nfrom simple.code.system_prompts import MD_HEADING, load_message_template\\nfrom simple.code.utils import extract_json_block, colored_print, Fore\\nfrom simple.code.code_execute import execute_python_code, execute_tool\\n# from simple.code.logging_config import setup_logging # Assuming configured elsewhere\\n\\n# setup_logging()\\n\\nMAX_RETRIES = 3  # Reduced default retries to 3 for faster failure if stuck\\nJSON_ERROR_HINT = \"Invalid JSON. Provide a valid JSON object wrapped in triple backticks (```json ... ```).\"\\nUSE_KEY_ERROR_HINT = \"JSON requires a \\'use\\' key with value \\'python\\', \\'tool\\', or \\'respond\\'.\"\\nCHECK_KEY_ERROR_HINT = \"JSON requires a \\'use\\' key with value \\'yes\\' or \\'no\\'.\"\\n\\n\\nclass AgentExecutor:\\n    def __init__(self, model_name: str, messages_context: List[dict] = None,\\n                 agent_manager: AgentInteractionManager = None) -> None:\\n        self.model_name = model_name\\n        self.messages_context = messages_context if messages_context is not None else []\\n        self.summary: str = \"\"\\n        self.full_prompt: str = \"\"\\n        # Use the provided agent_manager or create a new one\\n        # Ensure AgentInteractionManager doesn\\'t require Tkinter root at init\\n        self.agent_manager = agent_manager if agent_manager is not None else AgentInteractionManager()\\n        self.current_retries = 0  # Track retries per execution cycle\\n\\n    def _build_correction_prompt(self, base_prompt: List[dict], bad_response: Any, hint: str) -> List[dict]:\\n        \"\"\"Appends the bad response and a correction hint to the prompt history.\"\"\"\\n        prompt = base_prompt.copy()\\n        # Ensure assistant message is properly formatted (string, potentially JSON string)\\n        assistant_content = json.dumps(bad_response) if not isinstance(\\n            bad_response, str) else bad_response\\n        prompt.append({\\'role\\': \\'assistant\\', \\'content\\': assistant_content})\\n        prompt.append({\\'role\\': \\'user\\', \\'content\\': hint})\\n        logging.warning(f\"Correction needed. Hint: {hint}\")\\n        return prompt\\n\\n    async def _get_valid_json_response(self, base_prompt: List[dict], scratchpad_widget: Any,\\n                                       root: Any, error_hint: str, max_retries: int = MAX_RETRIES) -> Tuple[Optional[Union[Dict, List]], str, List[dict]]:\\n        \"\"\"Attempts to get a valid JSON response from the model, with retries.\"\"\"\\n        retries = 0\\n        current_prompt = base_prompt\\n        while retries < max_retries:\\n            raw_response = run_inference(\\n                current_prompt, scratchpad_widget, root, self.model_name)\\n            try:\\n                # Use the robust extract_json_block function\\n                json_resp = extract_json_block(raw_response)\\n                # Optional: Add validation specific to expected JSON structure if needed\\n                # e.g., if isinstance(json_resp, dict) and \\'required_key\\' in json_resp:\\n                return json_resp, raw_response, current_prompt\\n            except ValueError as e:\\n                logging.warning(\\n                    f\"JSON extraction/validation failed (Attempt {retries + 1}/{max_retries}): {e}\")\\n                retries += 1\\n                current_prompt = self._build_correction_prompt(\\n                    current_prompt, raw_response, f\"{error_hint} Error: {e}\"\\n                )\\n            except Exception as e:  # Catch other potential errors during inference/parsing\\n                logging.error(\\n                    f\"Unexpected error getting JSON response (Attempt {retries + 1}/{max_retries}): {e}\")\\n                retries += 1\\n                current_prompt = self._build_correction_prompt(\\n                    current_prompt, raw_response, f\"Unexpected error occurred: {e}. {error_hint}\"\\n                )\\n\\n        logging.error(\\n            f\"Failed to get valid JSON response after {max_retries} retries.\")\\n        # Return None for JSON if all retries fail\\n        return None, raw_response, current_prompt\\n\\n    async def agent_execution(self, base_prompt: List[dict], output_widget: Any,\\n                              scratchpad_widget: Any, root: Any) -> Tuple[str, str]:\\n        \"\"\"Main agent execution loop: Decide -> Act (Python/Tool/Respond) -> Check -> Final Answer.\"\"\"\\n        logging.info(\"=== Starting Agent Execution Cycle ===\")\\n        action_log = \"\"\\n        self.current_retries = 0  # Reset retries for this cycle\\n\\n        while self.current_retries < MAX_RETRIES:\\n            logging.info(\\n                f\"--- Agent Step (Attempt {self.current_retries + 1}/{MAX_RETRIES}) ---\")\\n            # 1. Decide Action Type (Python, Tool, Respond)\\n            colored_print(\"Step 1: Deciding Action...\", Fore.CYAN)\\n            json_instruct, raw_response, current_prompt = await self._get_valid_json_response(\\n                base_prompt, scratchpad_widget, root, JSON_ERROR_HINT\\n            )\\n\\n            if json_instruct is None:\\n                final_response = \"Agent failed to decide on an action after multiple retries.\"\\n                action_log += f\"\\\\n[Decision Error]\\\\n{final_response}\\\\nRaw Response:\\\\n{raw_response}\"\\n                self.current_retries = MAX_RETRIES  # Break loop\\n                continue  # Go to end of loop\\n\\n            # Validate \\'use\\' key\\n            if not isinstance(json_instruct, dict) or \\'use\\' not in json_instruct:\\n                current_prompt = self._build_correction_prompt(\\n                    current_prompt, raw_response, USE_KEY_ERROR_HINT)\\n                self.current_retries += 1\\n                continue  # Retry decision\\n\\n            ai_choice = str(json_instruct.get(\"use\", \"\")).lower()\\n            if ai_choice not in [\\'python\\', \\'tool\\', \\'respond\\']:\\n                current_prompt = self._build_correction_prompt(\\n                    current_prompt, raw_response, USE_KEY_ERROR_HINT + f\" Got: \\'{ai_choice}\\'\")\\n                self.current_retries += 1\\n                continue  # Retry decision\\n\\n            # Append successful decision to prompt history\\n            # Use raw_response which contains the JSON\\n            current_prompt.append(\\n                {\\'role\\': \\'assistant\\', \\'content\\': raw_response})\\n            base_prompt = current_prompt  # Update base for next potential loop iteration\\n\\n            colored_print(f\"Decision: Use \\'{ai_choice}\\'\", Fore.BLUE)\\n            action_log += f\"\\\\n[Decision]\\\\nChoice: {ai_choice}\\\\nRationale (implied): {raw_response}\\\\n\"\\n\\n            # 2. Execute Action\\n            final_response = \"\"\\n            execution_successful = False\\n\\n            if ai_choice == \"python\":\\n                colored_print(\"Step 2: Executing Python Code...\", Fore.CYAN)\\n                self.agent_manager.move_to_python()\\n                py_base_prompt = load_message_template(\\n                    \\'python\\', self.summary)  # Pass summary\\n                # Add original user request to the python prompt context\\n                py_base_prompt.append(\\n                    {\\'role\\': \\'user\\', \\'content\\': f\"Based on the request:\\\\n\\'{self.full_prompt}\\'\\\\nGenerate the necessary Python code.\"})\\n                # Append previous steps from the main prompt if useful context exists? (Careful not to make it too long)\\n                # py_base_prompt.extend(current_prompt[-4:]) # Add last few turns for context?\\n\\n                exec_response, code_script, status_message, py_base_prompt_result = await self.code_use(\\n                    py_base_prompt, scratchpad_widget, root)\\n\\n                action_log += (\\n                    f\"\\\\n[Python Code Execution]\\\\n\"\\n                    f\"Code:\\\\n```python\\\\n{\\'\\'.join(code_script)}\\\\n```\\\\n\"\\n                    f\"Status: {status_message}\\\\n\"\\n                    f\"Result/Response: {exec_response}\\\\n\"\\n                )\\n                if status_message != \"Max retries reached.\":  # Check for success based on message content\\n                    execution_successful = True\\n                    final_response = exec_response  # Use the response generated after code execution\\n\\n            elif ai_choice == \"tool\":\\n                colored_print(\"Step 2: Executing Tool...\", Fore.CYAN)\\n                self.agent_manager.move_to_tool()\\n                tool_base_prompt = load_message_template(\\n                    \\'tool\\', self.summary)  # Pass summary\\n                tool_base_prompt.append(\\n                    {\\'role\\': \\'user\\', \\'content\\': f\"Based on the request:\\\\n\\'{self.full_prompt}\\'\\\\nSelect and configure the appropriate tool.\"})\\n                # tool_base_prompt.extend(current_prompt[-4:]) # Context?\\n\\n                exec_response, tool_instruct, status_messages, tool_base_prompt_result = await self.tool_use(\\n                    tool_base_prompt, scratchpad_widget, root)\\n\\n                status_msg_str = \"\\\\n\".join(status_messages)\\n                action_log += (\\n                    f\"\\\\n[Tool Execution]\\\\n\"\\n                    f\"Instruction:\\\\n```json\\\\n{json.dumps(tool_instruct, indent=2)}\\\\n```\\\\n\"\\n                    f\"Status: {status_msg_str}\\\\n\"\\n                    f\"Result/Response: {exec_response}\\\\n\"\\n                )\\n                if \"Max retries reached.\" not in status_msg_str:  # Check for success\\n                    execution_successful = True\\n                    final_response = exec_response  # Use the response generated after tool execution\\n\\n            elif ai_choice == \"respond\":\\n                colored_print(\\n                    \"Step 2: Generating Direct Response...\", Fore.CYAN)\\n                solve_base_prompt = load_message_template(\\n                    \\'answer\\', self.summary)\\n                solve_base_prompt.append(\\n                    {\\'role\\': \\'user\\', \\'content\\': f\"Provide a direct final answer to the following request:\\\\n\\'{self.full_prompt}\\'?\"}\\n                    # Optional: Include context from current_prompt if needed\\n                    # solve_base_prompt.extend(current_prompt[-4:])\\n                )\\n                final_response = run_inference(\\n                    solve_base_prompt, output_widget, root, self.model_name)\\n                action_log += f\"\\\\n[Direct Response]\\\\nAnswer: {final_response}\\\\n\"\\n                # Direct response always breaks the loop to return the answer.\\n                self.agent_manager.return_to_normal()  # Ensure agent returns visually\\n                return final_response, action_log\\n\\n            # Signal agent return after action attempt\\n            self.agent_manager.return_to_normal()\\n\\n            # If execution failed, retry the decision loop\\n            if not execution_successful:\\n                colored_print(\\n                    \"Execution failed, retrying decision...\", Fore.YELLOW)\\n                self.current_retries += 1\\n                # Add failure context for the next decision attempt\\n                base_prompt.append(\\n                    {\\'role\\': \\'user\\', \\'content\\': f\"The previous attempt to use \\'{ai_choice}\\' failed. Review the log:\\\\n{action_log}\\\\nReassess the best approach (\\'python\\', \\'tool\\', or \\'respond\\') to fulfill the request: \\'{self.full_prompt}\\'\"})\\n                continue  # Go to the start of the while loop\\n\\n            # 3. Check if the request is fulfilled\\n            colored_print(\\n                \"Step 3: Checking if Request Fulfilled...\", Fore.CYAN)\\n            check_prompt = load_message_template(\\'check\\', self.summary)\\n            check_prompt.append({\\n                \\'role\\': \\'user\\',\\n                \\'content\\': f\"Review the execution log:\\\\n{action_log}\\\\n\\\\nBased *only* on this log, was the original request \\'{self.full_prompt}\\' successfully fulfilled? Respond yes or no.\"\\n            })\\n\\n            # Get validated check response (yes/no)\\n            json_check, raw_check_response, check_prompt = await self._get_valid_json_response(\\n                check_prompt, scratchpad_widget, root, CHECK_KEY_ERROR_HINT\\n            )\\n\\n            if json_check is None:\\n                final_response = \"Agent failed to check the result after multiple retries.\"\\n                action_log += f\"\\\\n[Check Error]\\\\n{final_response}\\\\nRaw Response:\\\\n{raw_check_response}\"\\n                self.current_retries = MAX_RETRIES  # Break loop\\n                continue  # Go to end of loop\\n\\n            # Validate check response structure\\n            if not isinstance(json_check, dict) or \\'use\\' not in json_check:\\n                base_prompt = self._build_correction_prompt(\\n                    check_prompt, raw_check_response, CHECK_KEY_ERROR_HINT)\\n                self.current_retries += 1\\n                # Retry (will likely start from decision again based on main loop structure)\\n                continue\\n\\n            check_choice = str(json_check.get(\"use\", \"\")).lower()\\n            if check_choice not in [\\'yes\\', \\'no\\']:\\n                base_prompt = self._build_correction_prompt(\\n                    check_prompt, raw_check_response, CHECK_KEY_ERROR_HINT + f\" Got: \\'{check_choice}\\'\")\\n                self.current_retries += 1\\n                continue  # Retry\\n\\n            # Append check result to prompt history\\n            current_prompt = check_prompt  # Start from the check prompt context\\n            current_prompt.append(\\n                {\\'role\\': \\'assistant\\', \\'content\\': raw_check_response})\\n            base_prompt = current_prompt  # Update base prompt\\n\\n            action_log += f\"\\\\n[Check Result]\\\\nFulfilled: {check_choice}\\\\n\"\\n            colored_print(f\"Request fulfilled: {check_choice}\", Fore.BLUE)\\n\\n            # 4. Final Answer Generation or Loop Continuation\\n            if check_choice == \"yes\":\\n                colored_print(\"Step 4: Generating Final Answer...\", Fore.CYAN)\\n                solve_base_prompt = load_message_template(\\n                    \\'answer\\', self.summary)\\n                solve_base_prompt.append({\\n                    \\'role\\': \\'user\\',\\n                    \\'content\\': f\"Based on the following execution log:\\\\n{action_log}\\\\n\\\\nProvide the final, comprehensive answer to the original request:\\\\n\\'{self.full_prompt}\\'\"\\n                })\\n                final_answer = run_inference(\\n                    solve_base_prompt, output_widget, root, self.model_name)\\n                action_log += f\"\\\\n[Final Answer]\\\\n{final_answer}\\\\n\"\\n                logging.info(\\n                    \"=== Agent Execution Cycle Completed Successfully ===\")\\n                return final_answer, action_log\\n            else:\\n                # If not fulfilled, add context and loop back to decision step\\n                colored_print(\\n                    \"Request not fulfilled, continuing execution...\", Fore.YELLOW)\\n                self.current_retries += 1\\n                base_prompt.append(\\n                    {\\'role\\': \\'user\\', \\'content\\': f\"The request \\'{self.full_prompt}\\' was not fulfilled. Review the log:\\\\n{action_log}\\\\nDecide the *next* step (\\'python\\', \\'tool\\', or \\'respond\\').\"})\\n                # Continue to the next iteration of the while loop\\n\\n        # If loop finishes due to max retries\\n        logging.error(f\"Agent execution failed after {MAX_RETRIES} attempts.\")\\n        final_response = final_response or \"Agent execution failed to complete after maximum retries.\"\\n        action_log += \"\\\\n[Execution Failed]\\\\nAgent stopped after maximum retries.\\\\n\"\\n        return final_response, action_log\\n\\n    async def tool_use(self, base_prompt: List[dict], scratchpad_widget: Any, root: Any) -> Tuple[str, Dict, List[str], List[dict]]:\\n        \"\"\"Handles the process of selecting, executing, and responding to a tool use.\"\"\"\\n        retries = 0\\n        current_prompt = base_prompt\\n        tool_instruct: Dict = {}\\n        status_messages: List[str] = [\"Tool use not attempted.\"]\\n\\n        while retries < MAX_RETRIES:\\n            logging.info(\\n                f\"--- Tool Use Step (Attempt {retries + 1}/{MAX_RETRIES}) ---\")\\n            # 1. Get Tool Instruction\\n            json_resp, raw_response, current_prompt = await self._get_valid_json_response(\\n                current_prompt, scratchpad_widget, root,\\n                \"Provide a JSON object with \\'tool\\' and \\'parameters\\' keys, wrapped in ```json.\",\\n                max_retries=1  # Only one try to get the instruction per outer loop attempt\\n            )\\n\\n            if json_resp is None or not isinstance(json_resp, dict) or \\'tool\\' not in json_resp:\\n                logging.warning(\"Failed to get valid tool instruction JSON.\")\\n                status_messages = [\\n                    \"Failed to generate valid tool instruction.\"]\\n                # Build correction prompt for the *next* outer loop iteration if needed\\n                current_prompt = self._build_correction_prompt(\\n                    current_prompt, raw_response,\\n                    \"Invalid tool instruction. Provide a JSON object with \\'tool\\' and \\'parameters\\' keys.\"\\n                )\\n                retries += 1  # Consume a retry for the tool_use cycle\\n                continue  # Try getting instruction again in next loop\\n\\n            tool_instruct = json_resp  # Got valid instruction structure\\n            # Append successful instruction generation\\n            current_prompt.append(\\n                {\\'role\\': \\'assistant\\', \\'content\\': raw_response})\\n\\n            # 2. Execute Tool\\n            colored_print(\\n                f\"Executing Tool: {tool_instruct.get(\\'tool\\')}...\", Fore.GREEN)\\n            status_result = execute_tool(\\n                tool_instruct, self.agent_manager)  # Pass manager\\n            status_messages = [status_result.get(\\n                \\'message\\', \\'No message provided.\\')]\\n            self.agent_manager.update_status_msg(\\n                # Update visual status\\n                f\"Tool {tool_instruct.get(\\'tool\\')}: {status_messages[0][:100]}...\")\\n\\n            # 3. Handle Execution Result\\n            if status_result.get(\\'status\\') != \"200\":\\n                logging.warning(f\"Tool execution failed: {status_messages[0]}\")\\n                colored_print(f\"Tool failed: {status_messages[0]}\", Fore.RED)\\n                # Build correction prompt for the *next* attempt\\n                current_prompt.append({\\n                    \\'role\\': \\'user\\',\\n                    \\'content\\': f\"Tool execution failed.\\\\nInstruction:\\\\n```json\\\\n{json.dumps(tool_instruct, indent=2)}\\\\n```\\\\nError:\\\\n{status_messages[0]}\\\\n\\\\nPlease correct the tool instruction or parameters and try again.\"\\n                })\\n                retries += 1\\n                continue  # Retry getting instruction/executing\\n            else:\\n                # Execution successful\\n                colored_print(\\n                    f\"Tool executed successfully. Result: {status_messages[0]}\", Fore.GREEN)\\n                final_prompt = current_prompt.copy()\\n                final_prompt.append({\\n                    \\'role\\': \\'user\\',\\n                    \\'content\\': f\"Tool execution successful.\\\\nInstruction:\\\\n```json\\\\n{json.dumps(tool_instruct, indent=2)}\\\\n```\\\\nResult:\\\\n{status_messages[0]}\\\\n\\\\nProvide a response summarizing the result or indicating the next step.\"\\n                })\\n                final_response = run_inference(\\n                    final_prompt, scratchpad_widget, root, self.model_name)\\n                # Append the final summarization to the ongoing prompt history\\n                current_prompt.append(\\n                    {\\'role\\': \\'assistant\\', \\'content\\': final_response})\\n                return final_response, tool_instruct, status_messages, current_prompt  # Success\\n\\n        # If loop finishes due to retries\\n        logging.error(\"Tool use failed after maximum retries.\")\\n        return (\"Tool use failed after maximum retries.\", tool_instruct, [\"Max retries reached.\"], current_prompt)\\n\\n    async def code_use(self, base_prompt: List[dict], scratchpad_widget: Any, root: Any) -> Tuple[str, List[str], str, List[dict]]:\\n        \"\"\"Handles the process of generating, executing, and responding to Python code use.\"\"\"\\n        retries = 0\\n        current_prompt = base_prompt\\n        code_script: List[str] = []\\n        status_message: str = \"Code use not attempted.\"\\n\\n        while retries < MAX_RETRIES:\\n            logging.info(\\n                f\"--- Code Use Step (Attempt {retries + 1}/{MAX_RETRIES}) ---\")\\n            # 1. Generate Code\\n            raw_response = run_inference(\\n                current_prompt, scratchpad_widget, root, self.model_name)\\n\\n            try:\\n                # Extract potentially multiple blocks and join them\\n                extracted_blocks = self.extract_code_blocks(\\n                    raw_response, \\'python\\')\\n                if not extracted_blocks:\\n                    raise ValueError(\"No Python code blocks found.\")\\n                # Join blocks into one script string\\n                code_script = [\"\\\\n\".join(extracted_blocks)]\\n\\n            except ValueError as e:\\n                logging.warning(f\"Code extraction failed: {e}\")\\n                current_prompt = self._build_correction_prompt(\\n                    current_prompt, raw_response,\\n                    f\"No valid Python code blocks (```python ... ```) found. Please provide the code. Error: {e}\"\\n                )\\n                retries += 1\\n                continue  # Retry generation\\n\\n            # Append successful generation\\n            current_prompt.append(\\n                {\\'role\\': \\'assistant\\', \\'content\\': raw_response})\\n\\n            # 2. Execute Code\\n            colored_print(\\n                f\"Executing Python Code:\\\\n```python\\\\n{code_script[0][:200]}...\\\\n```\", Fore.GREEN)\\n            status_result = execute_python_code(\\n                \"\".join(code_script))  # Execute the joined script\\n            status_message = status_result.get(\\n                \\'message\\', \\'No message provided.\\')\\n            self.agent_manager.set_generated_code(\\n                \"\".join(code_script))  # For display in Python mode\\n            self.agent_manager.update_status_msg(\\n                f\"Python: {status_message[:100]}...\")  # Update visual status\\n\\n            # 3. Handle Execution Result\\n            if status_result.get(\\'status\\') != \"200\":\\n                logging.warning(f\"Code execution failed: {status_message}\")\\n                colored_print(f\"Code failed: {status_message}\", Fore.RED)\\n                # Build correction prompt for the *next* attempt\\n                current_prompt.append({\\n                    \\'role\\': \\'user\\',\\n                    \\'content\\': f\"Code execution failed.\\\\nCode:\\\\n```python\\\\n{\\'\\'.join(code_script)}\\\\n```\\\\nError:\\\\n{status_message}\\\\n\\\\nPlease correct the code and try again.\"\\n                })\\n                retries += 1\\n                continue  # Retry generation/execution\\n            else:\\n                # Execution successful\\n                colored_print(\\n                    f\"Code executed successfully. Result: {status_message}\", Fore.GREEN)\\n                final_prompt = current_prompt.copy()\\n                final_prompt.append({\\n                    \\'role\\': \\'user\\',\\n                    \\'content\\': f\"Code execution successful.\\\\nCode:\\\\n```python\\\\n{\\'\\'.join(code_script)}\\\\n```\\\\nResult:\\\\n{status_message}\\\\n\\\\nProvide a response summarizing the result or indicating the next step.\"\\n                })\\n                final_response = run_inference(\\n                    final_prompt, scratchpad_widget, root, self.model_name)\\n                # Append the final summarization to the ongoing prompt history\\n                current_prompt.append(\\n                    {\\'role\\': \\'assistant\\', \\'content\\': final_response})\\n                return final_response, code_script, status_message, current_prompt  # Success\\n\\n        # If loop finishes due to retries\\n        logging.error(\"Code use failed after maximum retries.\")\\n        return (\"Code use failed after maximum retries.\", code_script, \"Max retries reached.\", current_prompt)\\n\\n    async def decide_execution(self, text_prompt: dict, output_widget: Any,\\n                               scratchpad_widget: Any, root: Any, full_prompt: str) -> Tuple[str, str]:\\n        \"\"\"Entry point for agent execution based on a user prompt.\"\"\"\\n        self.full_prompt = full_prompt\\n        # Update model name from GUI if changed\\n        self.agent_executor.model_name = self.model_var.get()\\n        # Load base template, add summary context, then add user prompt\\n        base_prompt = load_message_template(\\'base\\', self.summary)\\n        # Append {\\'role\\': \\'user\\', \\'content\\': full_prompt}\\n        base_prompt.append(text_prompt)\\n        return await self.agent_execution(base_prompt, output_widget, scratchpad_widget, root)\\n\\n    @staticmethod\\n    def extract_code_blocks(text: str, language: str) -> List[str]:\\n        \"\"\"Extracts code blocks for a specified language, handling optional language labels.\"\"\"\\n        # Pattern looks for ``` optionally followed by the language, newline, then captures content until ```\\n        pattern = rf\"```{re.escape(language)}\\\\s*\\\\n(.*?)\\\\n```|```\\\\n(.*?)\\\\n```\"\\n        try:\\n            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\\n            # findall returns tuples for groups, filter out empty strings and flatten\\n            blocks = [block for match in matches for block in match if block]\\n            if not blocks:\\n                # Fallback: Try finding just the content between any triple backticks if language specific fails\\n                pattern_any = r\"```(?:\\\\w*\\\\s*)?\\\\n(.*?)\\\\n```\"\\n                matches_any = re.findall(pattern_any, text, re.DOTALL)\\n                if matches_any:\\n                    logging.warning(\\n                        f\"Found code blocks but language \\'{language}\\' mismatch or missing. Using generic blocks.\")\\n                    blocks = [block for block in matches_any if block]\\n\\n            if not blocks:\\n                logging.warning(\\n                    f\"No code blocks found for language \\'{language}\\' or generic ``` blocks.\")\\n                # Optional: Could try parsing the whole text if it looks like code, but risky\\n                # if language == \\'python\\' and (\\'def \\' in text or \\'import \\' in text): return [text]\\n\\n            return blocks\\n        except Exception as e:\\n            logging.error(f\"Error extracting code blocks: {e}\")\\n            return []\\n'},\n",
              " {'simple\\\\code\\\\code_execute.py': '# ./simple/code/code_execute.py:\\nimport tempfile\\nimport subprocess\\nimport os\\nimport sys\\nimport logging\\nimport traceback  # Keep this import\\n\\n# from simple.code.logging_config import setup_logging # Assuming configured elsewhere\\n# setup_logging()\\n\\nEXECUTION_TIMEOUT_SECONDS = 60  # Define timeout constant\\n\\n\\ndef execute_python_code(code: str) -> dict:\\n    \"\"\"\\n    Executes a string of Python code in a separate process using a temporary file.\\n\\n    Args:\\n        code (str): The Python code to execute.\\n\\n    Returns:\\n        dict: A dictionary containing:\\n              - \"status\" (str): \"200\" for success, \"500\" for error, \"TIMEOUT\" for timeout.\\n              - \"message\" (str): Execution result (stdout) or error details (stderr, traceback).\\n    \"\"\"\\n    result = {\"status\": \"500\",\\n        \"message\": \"Execution did not complete.\"}  # Default to error\\n    temp_filename = None\\n\\n    # Ensure code is not empty\\n    if not code or not code.strip():\\n        result[\\'message\\'] = \"Execution failed: No code provided.\"\\n        logging.warning(\"execute_python_code called with empty code string.\")\\n        return result\\n\\n    try:\\n        # Create a temporary file with .py suffix\\n        with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".py\", encoding=\\'utf-8\\') as temp_file:\\n            temp_file.write(code)\\n            temp_filename = temp_file.name\\n        logging.info(f\"Executing code in temporary file: {temp_filename}\")\\n\\n        # Run the script using the same Python interpreter that runs this script\\n        completed = subprocess.run(\\n            [sys.executable, temp_filename],\\n            capture_output=True,\\n            text=True,  # Decodes stdout/stderr as text\\n            encoding=\\'utf-8\\',  # Explicitly set encoding\\n            timeout=EXECUTION_TIMEOUT_SECONDS  # Use the defined timeout\\n        )\\n\\n        # Check execution outcome\\n        if completed.returncode == 0:\\n            result[\\'status\\'] = \"200\"\\n            stdout = completed.stdout.strip()\\n            result[\\'message\\'] = f\"Execution successful.\\\\nOutput:\\\\n{stdout}\" if stdout else \"Execution successful.\\\\nOutput:\\\\n[No output]\"\\n            # Log snippet\\n            logging.info(\\n                f\"Code execution successful. Output: {stdout[:200]}...\")\\n        else:\\n            result[\\'status\\'] = \"500\"\\n            stderr = completed.stderr.strip()\\n            result[\\'message\\'] = f\"Execution failed with exit code {completed.returncode}.\\\\nError:\\\\n{stderr}\"\\n            logging.error(\\n                f\"Code execution failed (Exit Code {completed.returncode}). Error: {stderr}\")\\n\\n    except subprocess.TimeoutExpired:\\n        result[\\'status\\'] = \"TIMEOUT\"\\n        result[\\'message\\'] = f\"Execution timed out after {EXECUTION_TIMEOUT_SECONDS} seconds.\"\\n        logging.error(result[\\'message\\'])\\n    except FileNotFoundError:\\n        result[\\'status\\'] = \"500\"\\n        result[\\'message\\'] = f\"Execution failed: Python interpreter not found at \\'{sys.executable}\\'.\"\\n        logging.critical(result[\\'message\\'])\\n    except Exception as e:\\n        result[\\'status\\'] = \"500\"\\n        # Include traceback for better debugging internal errors\\n        result[\\'message\\'] = f\"Execution failed due to an unexpected error: {str(e)}\\\\n{traceback.format_exc()}\"\\n        logging.error(result[\\'message\\'])\\n    finally:\\n        # Clean up the temporary file\\n        if temp_filename and os.path.exists(temp_filename):\\n            try:\\n                os.remove(temp_filename)\\n                logging.info(f\"Removed temporary file: {temp_filename}\")\\n            except OSError as e:\\n                # Log error but don\\'t overwrite the primary result\\n                logging.error(\\n                    f\"Error removing temporary file {temp_filename}: {e}\")\\n            except Exception as e:  # Catch any other potential permission issues etc.\\n                logging.error(\\n                    f\"Unexpected error removing temporary file {temp_filename}: {e}\")\\n\\n    return result\\n\\n\\ndef execute_tool(instruction: dict, agent_manager) -> dict:\\n    \"\"\"\\n    Executes a registered tool function based on the instruction dictionary.\\n\\n    Args:\\n        instruction (dict): Expected to have \\'tool\\' (str) and \\'parameters\\' (any, passed to tool).\\n        agent_manager (AgentInteractionManager): Instance to control the visual agent.\\n\\n    Returns:\\n        dict: A dictionary containing:\\n              - \"status\" (str): \"200\" for success, \"500\" for error.\\n              - \"message\" (str): Tool output or error details.\\n    \"\"\"\\n    result = {\"status\": \"500\", \"message\": \"Tool execution did not complete.\"}\\n\\n    if not isinstance(instruction, dict):\\n        result[\\'message\\'] = \"Error: Instruction must be a dictionary.\"\\n        logging.error(result[\\'message\\'])\\n        return result\\n\\n    tool_name = instruction.get(\\'tool\\')\\n    if not tool_name or not isinstance(tool_name, str):\\n        result[\\'message\\'] = \"Error: \\'tool\\' key (string) missing or invalid in instruction.\"\\n        logging.error(f\"{result[\\'message\\']} Instruction: {instruction}\")\\n        return result\\n\\n    # Assuming tool_registry is globally accessible or imported correctly\\n    from simple.code.tool_registry import tool_registry\\n\\n    tool_func = tool_registry.get(tool_name)\\n    if not tool_func or not callable(tool_func):\\n        result[\\'message\\'] = f\"Error: Tool \\'{tool_name}\\' not found or is not callable.\"\\n        logging.error(\\n            f\"{result[\\'message\\']} Available tools: {list(tool_registry.keys())}\")\\n        return result\\n\\n    # Move agent visually before execution\\n    agent_manager.move_to_tool_dynamic(tool_name)\\n\\n    # Parameters can be anything the tool expects\\n    params = instruction.get(\\'parameters\\')\\n    logging.info(f\"Executing tool \\'{tool_name}\\' with parameters: {params}\")\\n\\n    try:\\n        # Execute the tool function with the provided parameters\\n        # Consider using inspect module if args/kwargs need validation, but for now pass directly\\n        if params is not None:\\n            output = tool_func(params)\\n        else:\\n            # Handle tools that might not expect parameters\\n            # Check function signature? For now, assume it handles None or no args if params is None.\\n            # This might require tools to be designed to handle params=None gracefully.\\n            try:\\n                 output = tool_func()  # Try calling without args if None provided\\n            except TypeError:\\n                 # If it fails, maybe it required args. Re-raise or handle specific tool needs.\\n                logging.warning(f\"Tool \\'{tool_name}\\' called without parameters, but might require them.\")\\n                output = tool_func(params)  # Retry with None, maybe it handles it internally\\n\\n        # Check tool output (some tools might intentionally return None on success)\\n        # We rely on exceptions for failure indication primarily.\\n        output_str = str(output)  # Convert result to string for the message\\n        result[\\'status\\'] = \"200\"\\n        result[\\'message\\'] = f\"Tool \\'{tool_name}\\' executed successfully.\\\\nResult:\\\\n{output_str}\"\\n        logging.info(\\n            f\"Tool \\'{tool_name}\\' successful. Result: {output_str[:200]}...\")\\n\\n    except Exception as e:\\n        # Catch exceptions during tool execution\\n        error_details = f\"Error executing tool \\'{tool_name}\\': {str(e)}\\\\n{traceback.format_exc()}\"\\n        result[\\'status\\'] = \"500\"\\n        result[\\'message\\'] = f\"Tool execution failed:\\\\n{error_details}\"\\n        logging.error(error_details)\\n\\n    return result\\n'},\n",
              " {'simple\\\\code\\\\function_call.py': '# ./simple/code/function_call.py:\\nimport tempfile\\nimport subprocess\\nimport os\\nimport logging\\nimport shlex\\nimport traceback  # Import traceback\\nfrom typing import Any, Union, List\\n\\n# from simple.code.logging_config import setup_logging # Assuming configured elsewhere\\n# setup_logging()\\n\\n# Security Warning: Executing arbitrary bash commands is highly risky.\\n# Consider replacing this with specific Python functions or heavily sanitizing/validating input.\\n\\n\\ndef execute_bash_command(command: str) -> str:\\n    \"\"\"\\n    Executes a bash command provided as a string.\\n\\n    Args:\\n        command: The command to execute.\\n                 - If string: Treated as a single command line, split by shlex.\\n\\n    Returns:\\n        str: The standard output of the command if successful.\\n\\n    Raises:\\n        ValueError: If the command format is unsupported or invalid.\\n        Exception: If the command fails to execute (non-zero exit code or other error).\\n                  The exception message includes stderr.\\n    \"\"\"\\n    logging.warning(\\n        \"Executing bash command. Ensure the command source is trusted and validated.\")\\n    run_command_list: List[str] = []\\n\\n    try:\\n        if isinstance(command, str):\\n            if not command.strip():\\n                raise ValueError(\"Command string cannot be empty.\")\\n            run_command_list = shlex.split(command)\\n        elif isinstance(command, list):\\n            if not command:\\n                raise ValueError(\"Command list cannot be empty.\")\\n            # Ensure all elements are strings\\n            if not all(isinstance(item, str) for item in command):\\n                raise ValueError(\\n                    \"All elements in command list must be strings.\")\\n            run_command_list = command\\n        elif isinstance(command, dict):\\n            cmd_str = command.get(\\'command\\') or command.get(\\'bash_command\\')\\n            if not cmd_str or not isinstance(cmd_str, str) or not cmd_str.strip():\\n                raise ValueError(\\n                    \"Command dictionary must contain a non-empty string value for \\'command\\' or \\'bash_command\\' key.\")\\n            run_command_list = shlex.split(cmd_str)\\n        else:\\n            raise ValueError(\\n                f\"Unsupported command format: {type(command)}. Use str, list, or dict.\")\\n\\n        if not run_command_list:\\n            raise ValueError(\"Derived command list is empty after parsing.\")\\n\\n        logging.info(f\"Executing bash command: {run_command_list}\")\\n        # Use text=True for automatic decoding, explicitly set encoding\\n        completed = subprocess.run(\\n            run_command_list,\\n            capture_output=True,\\n            text=True,\\n            encoding=\\'utf-8\\',\\n            check=False  # Don\\'t raise CalledProcessError automatically, check returncode manually\\n        )\\n\\n        if completed.returncode != 0:\\n            stderr = completed.stderr.strip()\\n            error_message = (f\"Command \\'{\\' \\'.join(run_command_list)}\\' failed with exit code {completed.returncode}\"\\n                             f\"{f\\', stderr: {stderr}\\' if stderr else \\'\\'}\")\\n            logging.error(error_message)\\n            # Raise an exception that includes the error details\\n            raise Exception(error_message)\\n        else:\\n            stdout = completed.stdout.strip()\\n            logging.info(f\"Bash command successful. Output: {stdout[:200]}...\")\\n            return stdout\\n\\n    except FileNotFoundError:\\n        # Error if the command itself (e.g., \\'ls\\') isn\\'t found\\n        error_message = f\"Error executing bash command: Command \\'{run_command_list[0]}\\' not found.\"\\n        logging.error(error_message)\\n        raise Exception(error_message) from None  # Raise clean exception\\n    except ValueError as ve:  # Catch specific ValueErrors from validation\\n        logging.error(f\"Invalid command input: {ve}\")\\n        raise  # Re-raise the ValueError\\n    except Exception as e:\\n        # Catch other potential errors (like subprocess issues, permissions)\\n        # Avoid catching the Exception raised for non-zero return code if already logged\\n        if \"failed with exit code\" not in str(e):\\n            error_message = f\"Error executing bash command \\'{run_command_list}\\': {e}\\\\n{traceback.format_exc()}\"\\n            logging.error(error_message)\\n            # Wrap unexpected errors in a standard Exception\\n            raise Exception(error_message) from e\\n        else:\\n            raise  # Re-raise the command failure exception\\n\\n\\ndef write_custom_python_file(file_path: str, code: str) -> str:\\n    \"\"\"\\n    Writes the provided Python code to a file within the \\'simple/code/custom\\' directory.\\n    Prevents path traversal attacks.\\n\\n    Args:\\n        file_path (str): The desired filename (e.g., \"my_tool.py\"). Should not contain path separators.\\n        code (str): The Python code content to write.\\n\\n    Returns:\\n        str: The full path to the written file if successful.\\n\\n    Raises:\\n        ValueError: If file_path is invalid or attempts path traversal.\\n        OSError: If file writing fails.\\n    \"\"\"\\n    base_folder = os.path.abspath(os.path.join(\"simple\", \"code\", \"custom\"))\\n\\n    # Basic sanitization: remove leading/trailing whitespace and path separators\\n    # This is a simple check; more robust validation might be needed depending on usage.\\n    clean_filename = os.path.basename(file_path.strip())\\n\\n    if not clean_filename or clean_filename != file_path.strip():\\n        raise ValueError(\\n            f\"Invalid file_path \\'{file_path}\\'. It should be a simple filename without path separators.\")\\n\\n    # Ensure the filename ends with .py (optional, but good practice for Python tools)\\n    # if not clean_filename.lower().endswith(\\'.py\\'):\\n    #     clean_filename += \\'.py\\' # Append if missing\\n\\n    full_path = os.path.join(base_folder, clean_filename)\\n\\n    # Final check: ensure the resolved path is still within the intended base folder\\n    if os.path.commonpath([base_folder, os.path.abspath(full_path)]) != base_folder:\\n        raise ValueError(\\n            f\"Security risk: Invalid file path \\'{file_path}\\' attempts to write outside the allowed directory.\")\\n\\n    try:\\n        os.makedirs(base_folder, exist_ok=True)  # Ensure directory exists\\n        with open(full_path, \"w\", encoding=\"utf-8\") as f:\\n            f.write(code)\\n        logging.info(f\"File successfully written to: {full_path}\")\\n        print(f\"File successfully written to: {full_path}\")\\n        return full_path\\n    except OSError as e:\\n        logging.error(f\"Failed to write file {full_path}: {e}\")\\n        raise  # Re-raise OSError\\n    except Exception as e:\\n        logging.error(\\n            f\"An unexpected error occurred while writing file {full_path}: {e}\\\\n{traceback.format_exc()}\")\\n        raise OSError(\\n            f\"Failed to write file {full_path} due to unexpected error.\") from e\\n\\n\\n# Example usage within __main__ remains largely the same\\nif __name__ == \"__main__\":\\n    try:\\n        file_path = \"example_tool.py\"  # Simple filename\\n        code_content = (\\n            \"import logging\\\\n\\\\n\"\\n            \"def sample_tool_function(params):\\\\n\"\\n            \"    \\'\\'\\'Example tool documentation.\\'\\'\\'\\\\n\"\\n            \"    logging.info(f\\'Sample tool executed with params: {params}\\')\\\\n\"\\n            \"    return f\\'Processed: {params}\\'\\\\n\"\\n        )\\n        written_path = write_custom_python_file(file_path, code_content)\\n\\n        # Example bash command (use with caution)\\n        # list_files_cmd = \"ls -l simple/code/custom\"\\n        # output = execute_bash_command(list_files_cmd)\\n        # print(f\"\\\\nBash command output for \\'{list_files_cmd}\\':\\\\n{output}\")\\n\\n        # Example invalid path\\n        # write_custom_python_file(\"../outside_tool.py\", \"# Malicious code\")\\n\\n    except (ValueError, OSError, Exception) as e:\\n        print(f\"\\\\nAn error occurred: {e}\")\\n'},\n",
              " {'simple\\\\code\\\\history.py': 'import os\\nimport json\\nfrom datetime import datetime\\nfrom typing import Any, List, Optional\\nimport logging\\nfrom simple.code.logging_config import setup_logging\\n\\n# Centralized logging setup\\nsetup_logging()\\n\\n\\nclass HistoryManager:\\n    def __init__(self, history_dir: str = \"simple/gag/history\") -> None:\\n        self.history_dir = history_dir\\n        os.makedirs(self.history_dir, exist_ok=True)\\n        self.current_file: Optional[str] = None\\n\\n    def get_history_files(self) -> List[str]:\\n        return [f for f in os.listdir(self.history_dir) if f.endswith(\".json\")]\\n\\n    def save_history(self, history_data: Any, filename: str = \"\") -> Optional[str]:\\n        if not filename:\\n            if not self.current_file:\\n                filename = f\"session_{datetime.now().strftime(\\'%Y%m%d_%H%M%S\\')}.json\"\\n            else:\\n                filename = self.current_file\\n        elif not filename.endswith(\".json\"):\\n            filename += \".json\"\\n        self.current_file = filename\\n        path = os.path.join(self.history_dir, filename)\\n        try:\\n            with open(path, \"w\", encoding=\"utf-8\") as f:\\n                json.dump(history_data, f, indent=4)\\n            logging.info(f\"History saved to {path}\")\\n            return path\\n        except Exception as e:\\n            logging.error(f\"Error saving history: {e}\")\\n            return None\\n\\n    def load_history(self, filename: str) -> Optional[Any]:\\n        path = os.path.join(self.history_dir, filename)\\n        try:\\n            with open(path, \"r\", encoding=\"utf-8\") as f:\\n                history = json.load(f)\\n            self.current_file = filename\\n            logging.info(f\"History loaded from {path}\")\\n            return history\\n        except Exception as e:\\n            logging.error(f\"Error loading history from {path}: {e}\")\\n            return None\\n\\n    def delete_history(self, filename: str) -> None:\\n        path = os.path.join(self.history_dir, filename)\\n        try:\\n            if os.path.exists(path):\\n                os.remove(path)\\n                logging.info(f\"Deleted history file: {path}\")\\n            else:\\n                logging.warning(f\"History file {path} does not exist.\")\\n        except Exception as e:\\n            logging.error(f\"Error deleting history file {path}: {e}\")\\n'},\n",
              " {'simple\\\\code\\\\inference.py': 'import asyncio\\nimport nest_asyncio\\nfrom typing import List, Any\\nfrom ollama import AsyncClient\\nimport logging\\nfrom simple.code.utils import colored_print, strip_model_escapes, Fore\\nfrom simple.code.logging_config import setup_logging\\n\\n# Apply nest_asyncio to allow nested event loops.\\nnest_asyncio.apply()\\n\\n# Centralized logging setup\\nsetup_logging()\\n\\ncurrent_client = None\\n\\n\\nclass InferenceClient:\\n    def __init__(self, model_name: str) -> None:\\n        self.model_name = model_name\\n        self.client = AsyncClient()\\n        self.cancelled = False\\n\\n    async def chat(self, messages: List[dict], widget: Any, root: Any) -> str:\\n        try:\\n            response_generator = await self.client.chat(model=self.model_name, messages=messages, stream=True)\\n        except Exception as e:\\n            logging.error(f\"Error starting chat: {e}\")\\n            return f\"Error starting inference: {e}\"\\n\\n        full_text = \"\"\\n\\n        def append_text(s: str) -> None:\\n            if widget is not None:\\n                widget.insert(\"end\", s)\\n                widget.see(\"end\")\\n\\n        # Use root.after if available; otherwise, print to console.\\n        if widget is not None and root is not None:\\n            root.after(0, append_text, \"\\\\n\")\\n        else:\\n            print(\"\\\\n\", end=\\'\\')\\n\\n        logging.info(\"Starting inference chat...\")\\n        async for part in response_generator:\\n            if self.cancelled:\\n                logging.info(\"Inference cancelled.\")\\n                if widget is not None and root is not None:\\n                    root.after(0, append_text, \"\\\\n[Inference cancelled]\")\\n                else:\\n                    print(\"\\\\n[Inference cancelled]\")\\n                break\\n            section = part.get(\"message\", {}).get(\"content\", \"\")\\n            clean_section = strip_model_escapes(section)\\n            full_text += clean_section\\n            print(clean_section, end=\\'\\', flush=True)\\n            if widget is not None and root is not None:\\n                root.after(0, append_text, clean_section)\\n            else:\\n                pass\\n        print()\\n        logging.info(\"Inference completed.\")\\n        return full_text\\n\\n    def cancel(self) -> None:\\n        self.cancelled = True\\n\\n\\ndef run_inference(messages: List[dict], widget: Any, root: Any, model_name: str) -> str:\\n    global current_client\\n    client = InferenceClient(model_name)\\n    current_client = client\\n    try:\\n        result = asyncio.run(client.chat(messages, widget, root))\\n    except Exception as e:\\n        logging.error(f\"Error during inference: {e}\")\\n        result = f\"Error during inference: {e}\"\\n    return result\\n'},\n",
              " {'simple\\\\code\\\\logging_config.py': 'import logging\\nimport re\\nimport os\\n\\n# Regular expression to match ANSI escape sequences.\\nANSI_ESCAPE = re.compile(r\\'\\\\x1B\\\\[[0-?]*[ -/]*[@-~]\\')\\n\\n\\nclass NoColorFormatter(logging.Formatter):\\n    \"\"\"\\n    Custom logging formatter that strips ANSI escape sequences from the message.\\n    \"\"\"\\n\\n    def format(self, record):\\n        original = super().format(record)\\n        return ANSI_ESCAPE.sub(\\'\\', original)\\n\\n\\ndef setup_logging():\\n    # Optionally, allow disabling color via an environment variable.\\n    disable_color = os.environ.get(\"DISABLE_COLOR_LOGS\", \"1\") == \"1\"\\n\\n    handler = logging.StreamHandler()\\n    if disable_color:\\n        formatter = NoColorFormatter(\\n            fmt=\\'%(asctime)s %(levelname)s %(message)s\\',\\n            datefmt=\\'%Y-%m-%d %H:%M:%S\\'\\n        )\\n    else:\\n        formatter = logging.Formatter(\\n            fmt=\\'%(asctime)s %(levelname)s %(message)s\\',\\n            datefmt=\\'%Y-%m-%d %H:%M:%S\\'\\n        )\\n    handler.setFormatter(formatter)\\n    logging.basicConfig(level=logging.INFO, handlers=[handler])\\n'},\n",
              " {'simple\\\\code\\\\memory.py': 'import ast\\nimport logging\\nimport requests\\nfrom tqdm import tqdm\\nimport chromadb\\nimport json\\nfrom simple.code.inference import run_inference\\nfrom simple.code.logging_config import setup_logging\\n\\n# Centralized logging setup\\nsetup_logging()\\nchroma_client = chromadb.Client()\\n\\n\\ndef get_or_create_collection(collection=\\'conversations\\'):\\n    return chroma_client.get_or_create_collection(name=collection)\\n\\n\\ndef embedText(writtenText, model=\\'nomic-embed-text\\'):\\n    payload = {\\n        \"model\": model,\\n        \"prompt\": writtenText,\\n        \"stream\": True,\\n        \"temperature\": 0,\\n        \"stop\": \"<|eot_id|>\"\\n    }\\n    try:\\n        request_response = requests.post(\\n            \"http://localhost:11434/api/embeddings\",\\n            headers={\"Content-Type\": \"application/json\"},\\n            data=json.dumps(payload),\\n            stream=True,\\n            timeout=1000  # Added timeout for network requests\\n        )\\n        decoded_list = None\\n        for line in request_response.iter_lines():\\n            if line:\\n                decoded_line = line.decode(\\'utf-8\\')\\n                decoded_dict = json.loads(decoded_line)\\n                decoded_list = decoded_dict[\\'embedding\\']\\n        return decoded_list\\n    except Exception as e:\\n        logging.error(f\"Error generating embedding: {e}\")\\n        return None\\n\\n\\ndef classify_embedding(query, context):\\n    classify_msg = (\\n        \\'You are an embedding classification AI agent. Your input will be a prompt and one embedded chunk of text. \\'\\n        \\'You will not respond as an AI assistant. You only respond \"yes\" or \"no\". \\'\\n        \\'Determine whether the context directly relates to the search query. \\'\\n        \\'If the context matches the search query requirements, respond \"yes\"; otherwise, respond \"no\".\\'\\n    )\\n    classify_convo = [\\n        {\\'role\\': \\'system\\', \\'content\\': classify_msg},\\n        {\\'role\\': \\'user\\', \\'content\\': \\'SEARCH QUERY: What is the users name? \\\\n\\\\nEMBEDDED CONTEXT: You are Ai Flexi. How can I help you today?\\'},\\n        {\\'role\\': \\'assistant\\', \\'content\\': \\'yes\\'},\\n        {\\'role\\': \\'user\\', \\'content\\': \\'SEARCH QUERY: Llama3 Python Voice Assistant \\\\n\\\\nEMBEDDED CONTEXT: Siri is a voice assistant on Apple iOS and Mac OS.\\'},\\n        {\\'role\\': \\'assistant\\', \\'content\\': \\'no\\'},\\n        {\\'role\\': \\'user\\', \\'content\\': f\\'SEARCH QUERY: {query} \\\\n\\\\nEMBEDDED CONTEXT: {context} \\'}\\n    ]\\n    classify_convo.append(\\n        {\\'role\\': \"user\", \\'content\\': f\\'SEARCH QUERY: {query} \\\\n\\\\nEMBEDDED CONTEXT: {context} \\'})\\n    response = run_inference(classify_convo, None, None, \"llama3.2\")\\n    return response.strip().strip().lower()\\n\\n\\ndef retrieve_embeddings(queries, db=\\'conversations\\', n_results=2):\\n    embeddings = set()\\n    for query in tqdm(queries, desc=\\'Retrieving Embeddings\\'):\\n        response = embedText(query)\\n        vector_db = get_or_create_collection(db)\\n        results = vector_db.query(\\n            query_embeddings=[response], n_results=n_results)\\n        best_embeddings = results[0]\\n        for best in best_embeddings:\\n            if best not in embeddings:\\n                if \\'yes\\' in classify_embedding(query, best):\\n                    embeddings.add(best)\\n    return embeddings\\n\\n\\ndef create_queries(prompt):\\n    query_message = (\\n        \\'You are a first principle reasoning search query AI agent. \\'\\n        \\'Generate a Python list of queries to search an embedding database for data needed to respond to the prompt. \\'\\n        \\'Return only a syntactically correct Python list.\\'\\n    )\\n    query_convo = [\\n        {\\'role\\': \\'system\\', \\'content\\': query_message},\\n        {\\'role\\': \\'user\\', \\'content\\': \\'Write an email to my car insurance company and request lower rates.\\'},\\n        {\\'role\\': \\'assistant\\',\\n            \\'content\\': \\'[\"What is the user\\\\\\'s name?\", \"Who is the current auto insurer?\", \"What is the current monthly rate?\"]\\'},\\n        {\\'role\\': \\'user\\', \\'content\\': \\'How can I convert the speak function in my llama3 python voice assistant to use pyttsx3 instead?\\'},\\n        {\\'role\\': \\'assistant\\',\\n            \\'content\\': \\'[\"Llama3 voice assistant\", \"Python voice assistant\", \"OpenAI TTS\", \"openai speak\"]\\'},\\n    ]\\n    query_convo.append({\\'role\\': \"user\", \\'content\\': prompt})\\n    response = run_inference(query_convo, None, None, \"llama3.2\")\\n    try:\\n        return ast.literal_eval(response)\\n    except Exception as e:\\n        logging.error(f\"Error parsing queries: {e}\")\\n        return [prompt]\\n\\n\\ndef create_vector_db(conversations, vector_db_name=\\'conversations\\'):\\n    vector_db_name = \\'conversations\\'\\n    try:\\n        chroma_client.delete_collection(name=vector_db_name)\\n    except ValueError:\\n        pass\\n    vector_db = chroma_client.create_collection(name=vector_db_name)\\n    for c in conversations:\\n        serialized_convo = f\\'prompt: {c[\"prompt\"]} response: {c[\"response\"]}\\'\\n        embeddedTextResult = embedText(serialized_convo)\\n        vector_db.add(ids=[f\"{c[\\'id\\']}\"], embeddings=[\\n                      embeddedTextResult], documents=[serialized_convo])\\n\\n\\ndef example_usage(user_input):\\n    queries = create_queries(user_input)\\n    embeddings = retrieve_embeddings(queries)\\n    prompt = f\\'MEMORIES: {embeddings}\\\\n\\\\nUSER PROMPT: {user_input}\\'\\n    response = run_inference(\\n        [{\\'role\\': \"user\", \\'content\\': prompt}], None, None, \"llama3.2\")\\n    return True\\n\\n\\ndef add_response_to_db(response: str, response_id: str, vector_db_name: str = \\'conversations\\'):\\n    \"\"\"\\n    Adds the AI\\'s valid response to the chromadb vector database.\\n\\n    Args:\\n        response (str): The AI\\'s valid response text.\\n        response_id (str): A unique identifier for this response.\\n        vector_db_name (str): The name of the vector DB collection.\\n    \"\"\"\\n    collection = get_or_create_collection(vector_db_name)\\n    embedding = embedText(response)\\n    if not embedding:\\n        logging.error(\"Failed to generate embedding for the response.\")\\n        return\\n    try:\\n        collection.add(ids=[response_id], embeddings=[\\n                       embedding], documents=[response])\\n        logging.info(\\n            f\"Successfully added response {response_id} to the vector db \\'{vector_db_name}\\'.\")\\n    except Exception as e:\\n        logging.error(f\"Error adding response to vector db: {e}\")\\n'},\n",
              " {'simple\\\\code\\\\system_prompts.py': 'import importlib.util\\nfrom typing import List, Dict\\nimport inspect\\nfrom simple.code.logging_config import setup_logging\\nfrom simple.code.tool_registry import tool_registry, get_tool_docs\\nsetup_logging()\\n\\nthesysname = \"You are FlexiAI.\"\\nDEFAULT_PROMPT_FILE = \"flexi.txt\"\\nDEFAULT_PROMPT_CONTENT = f\"{thesysname}. You think step by step, keeping key points in mind to solve or answer the request.\"\\nMD_HEADING = \"#\"\\nTRIPLE_BACKTICKS = \"```\"\\n\\n\\ndef add_context_to_messages(messages: List[Dict[str, str]], summary: str) -> List[Dict[str, str]]:\\n    \"\"\"\\n    Appends a summary context to each system message if provided.\\n    \"\"\"\\n    if summary:\\n        for msg in messages:\\n            if msg.get(\\'role\\') == \\'system\\':\\n                msg[\\'content\\'] += f\"\\\\n\\\\nSummary Context:\\\\n{summary}\"\\n    return messages\\n\\n\\ndef load_message_template(sys_type: str = \\'base\\', summary: str = \\'\\') -> List[Dict[str, str]]:\\n    sys_type = sys_type.lower()\\n    tool_docs = get_tool_docs()\\n    tool_names = list(tool_docs.keys())\\n    tool_documentation = [f\"{name}: {doc}\" for name, doc in tool_docs.items()]\\n\\n    sys_type = sys_type.lower()\\n    if sys_type == \"base\":\\n        content = f\"\"\"\\n{MD_HEADING} {thesysname}, an advanced AI agent capable of reflection, code execution and tool usage.\\n{MD_HEADING} You must handle user requests by reasoning step by step:\\n- 1) Understand the user request.\\n- 2) Choose if you will use a tool, python, or respond directly.\\n{MD_HEADING} Respond with your choice in a JSON object wrapped in triple backticks.\\n{MD_HEADING} Direct Response:\\n- If you can directly answer the user\\'s request without code or tools, select the \"respond\" use type.\\n{MD_HEADING} Usage:\\n- Provide a **JSON** response wrapped in triple backticks (starting with {TRIPLE_BACKTICKS}json).\\n- The response should contain the name of the executor to use (either \"python\", \"tool\", or \"respond\").\\n- Schema Examples:\\n    {TRIPLE_BACKTICKS}json\\n    {{\"use\": \"<name>\"}}\\n    {TRIPLE_BACKTICKS}\\n- Example 1:\\n    {TRIPLE_BACKTICKS}json\\n    {{\"use\": \"tool\"}}\\n    {TRIPLE_BACKTICKS}\\n- Example 2:\\n    {TRIPLE_BACKTICKS}json\\n    {{\"use\": \"python\"}}\\n    {TRIPLE_BACKTICKS}\\n- Example 3:\\n    {TRIPLE_BACKTICKS}json\\n    {{\"use\": \"respond\"}}\\n    {TRIPLE_BACKTICKS}\\n{MD_HEADING} Available Tools:\\n- Names: {\", \".join(tool_names)}\\n- Documentation:\\n{chr(10).join(tool_documentation)}\\n\"\"\"\\n        message = [{\\'role\\': \\'system\\', \\'content\\': content.strip()}]\\n    elif sys_type == \"answer\":\\n        content = f\"{MD_HEADING} {thesysname}, answer using only the provided information.\"\\n        message = [{\\'role\\': \\'system\\', \\'content\\': content.strip()}]\\n    elif sys_type == \"check\":\\n        content = f\"\"\"\\n{MD_HEADING} {thesysname}, determine if the user\\'s request was fulfilled.\\n{MD_HEADING} Respond in a JSON object wrapped in triple backticks.\\n{MD_HEADING} Example:\\n    {TRIPLE_BACKTICKS}json\\n    {{\"use\": \"yes\"}}\\n    {TRIPLE_BACKTICKS}\\n\"\"\"\\n        message = [{\\'role\\': \\'system\\', \\'content\\': content.strip()}]\\n    elif sys_type == \"tool\":\\n        content = f\"\"\"\\n{MD_HEADING} {thesysname}, an advanced AI agent capable of reflection and tool usage.\\n{MD_HEADING} You must handle user requests by reasoning step by step:\\n1) Understand the user request.\\n2) If a tool is needed, output a JSON instruction (wrapped in triple backticks).\\n3) Do not show any expected output.\\n4) If execution fails, adjust parameters and try again.\\n{MD_HEADING} Respond with your choice in a JSON object wrapped in triple backticks.\\n{MD_HEADING} Usage:\\n- Provide a **JSON** response wrapped in triple backticks (starting with {TRIPLE_BACKTICKS}json).\\n{MD_HEADING} Available Tools:\\n- Names: {\", \".join(tool_names)}\\n- Documentation:\\n{chr(10).join(tool_documentation)}\\n{MD_HEADING} Usage:\\n- Output a JSON response wrapped in triple backticks.\\n- Include the tool name and parameters.\\nExample:\\n    {TRIPLE_BACKTICKS}json\\n    {{\"tool\": \"<name>\", \"parameters\": \"<values>\"}}\\n    {TRIPLE_BACKTICKS}\\n\"\"\"\\n        message = [{\\'role\\': \\'system\\', \\'content\\': content.strip()}]\\n    elif sys_type == \"summary\":\\n        message = [{\\'role\\': \\'system\\', \\'content\\': f\"{MD_HEADING} You are a personal assistant. Extract and summarize key points from the provided text.  Focus on action items, entities and important facts.\"}]\\n    elif sys_type not in [\"tool\", ]:\\n        message = [\\n            {\\'role\\': \\'system\\',\\n                \\'content\\': f\\'{MD_HEADING} You are an expert {sys_type.capitalize()} Developer\\'}\\n        ]\\n    else:\\n        message = [\\n            {\\'role\\': \\'system\\', \\'content\\': f\"{MD_HEADING} You are an expert {sys_type.capitalize()} Developer.\"}]\\n    return add_context_to_messages(message, summary)\\n'},\n",
              " {'simple\\\\code\\\\tool_registry.py': 'import os\\nimport glob\\nimport ast\\nimport importlib.util\\nfrom typing import Dict, Callable\\n\\n\\ndef extract_functions_with_instruction(file_path: str) -> Dict[str, Callable]:\\n    with open(file_path, \\'r\\', encoding=\\'utf-8\\') as file:\\n        parsed = ast.parse(file.read(), file_path)\\n\\n    functions = {}\\n    for node in ast.walk(parsed):\\n        if isinstance(node, ast.FunctionDef):\\n            params = [arg.arg for arg in node.args.args]\\n            module_name = os.path.splitext(os.path.basename(file_path))[0]\\n            spec = importlib.util.spec_from_file_location(\\n                module_name, file_path)\\n            if spec and spec.loader:\\n                module = importlib.util.module_from_spec(spec)\\n                spec.loader.exec_module(module)\\n                func = getattr(module, node.name, None)\\n                if callable(func):\\n                    functions[node.name] = func\\n    return functions\\n\\n\\ndef load_custom_tools(custom_dir: str = os.path.join(\"simple\", \"code\", \"custom\")) -> Dict[str, Callable]:\\n    registry = {}\\n    py_files = glob.glob(os.path.join(custom_dir, \"*.py\"))\\n    for file in py_files:\\n        registry.update(extract_functions_with_instruction(file))\\n    return registry\\n\\n\\ndef load_builtin_tools() -> Dict[str, Callable]:\\n    builtin_path = os.path.join(\"simple\", \"code\", \"function_call.py\")\\n    return extract_functions_with_instruction(builtin_path)\\n\\n\\ndef get_tool_registry() -> Dict[str, Callable]:\\n    registry = load_builtin_tools()\\n    registry.update(load_custom_tools())\\n    return registry\\n\\n\\ntool_registry = get_tool_registry()\\n\\n\\ndef get_tool_docs():\\n    return {name: (func.__doc__ or \"No documentation provided\").strip()\\n            for name, func in tool_registry.items()}\\n'},\n",
              " {'simple\\\\code\\\\utils.py': 'import json\\nimport os\\nimport re\\nimport sys\\nfrom typing import Dict, List, Union\\nfrom colorama import Fore, Style\\nfrom simple.code.system_prompts import MD_HEADING\\n\\n# Pattern to remove model artifacts (e.g., \\'[0m\\')\\nESC_LIKE_PATTERN = re.compile(r\\'(?:)?\\\\[\\\\d+m\\')\\n# Check if the output supports color.\\nUSE_COLOR = sys.stdout.isatty()\\n\\n\\ndef strip_model_escapes(text: str) -> str:\\n    \"\"\"\\n    Removes model-generated sequences (like \\'[0m\\') that are not true ANSI escapes.\\n    \"\"\"\\n    return ESC_LIKE_PATTERN.sub(\\'\\', text)\\n\\n\\ndef colored_print(text: str, color: str = Fore.RESET, end: str = \"\\\\n\", flush: bool = False):\\n    \"\"\"\\n    Prints text in the specified color (if terminal supports it), then resets style.\\n    \"\"\"\\n    if USE_COLOR:\\n        print(f\"{color}{text}{Style.RESET_ALL}\", end=end, flush=flush)\\n    else:\\n        print(text, end=end, flush=flush)\\n\\n\\ndef get_py_files_recursive(directory: str, exclude_dirs: List[str] = None, exclude_files: List[str] = None) -> List[str]:\\n    \"\"\"\\n    Recursively finds Python files in a directory, excluding specified directories and files.\\n    \"\"\"\\n    if exclude_dirs is None:\\n        exclude_dirs = [\\'venv\\']\\n    if exclude_files is None:\\n        exclude_files = []\\n    py_files = []\\n    for root, dirs, files in os.walk(directory):\\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\\n        for file in files:\\n            if file.endswith(\\'.py\\') and file not in exclude_files:\\n                py_files.append(os.path.join(root, file))\\n    return py_files\\n\\n\\ndef read_file_content(path: str) -> str:\\n    \"\"\"\\n    Reads and returns the content of a file.\\n    \"\"\"\\n    try:\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            content = f.read()\\n        return content.replace(f\\'{MD_HEADING} \\', f\\'{MD_HEADING*2} \\')\\n    except Exception as e:\\n        print(f\"Error reading file {path}: {e}\")\\n        return \"\"\\n\\n\\ndef code_corpus(directory: str) -> List[str]:\\n    \"\"\"\\n    Builds a corpus by reading all Python files in the directory, excluding certain files and folders.\\n    \"\"\"\\n    exclude_files = []\\n    exclude_dirs = [\\'interest\\', \\'pyds\\', \\'backup\\', \\'models\\', \\'sdlc\\',\\n                    \\'self_autoCode\\', \\'self_autoCodebase\\', \\'tests\\', \\'to_confirm_tools\\', \\'node_modules\\', \\'\\']\\n    file_paths = get_py_files_recursive(\\n        directory, exclude_dirs=exclude_dirs, exclude_files=exclude_files)\\n    corpus = []\\n    for file_path in file_paths:\\n        try:\\n            text = read_file_content(file_path)\\n            corpus.append(f\\'\\\\n{MD_HEADING} {file_path}:\\\\n{text}\\\\n\\')\\n        except Exception as e:\\n            print(f\"Error processing {file_path}: {e}\")\\n    return corpus\\n\\n\\ndef extract_json_block(text: str) -> Union[Dict, List, str]:\\n    \"\"\"\\n    Extracts and returns the first valid JSON object or array found in the text.\\n    It searches for JSON blocks within triple backticks (with or without a language label)\\n    and also attempts to parse the entire text as JSON if no backticks are found.\\n\\n    Raises:\\n        ValueError: If no valid JSON block is found.\\n    \"\"\"\\n\\n    # First, try to find JSON within triple backticks (with or without language label)\\n    pattern = r\"```(?:\\\\w*\\\\n)?(.*?)```\"\\n    matches = re.findall(pattern, text, re.DOTALL)\\n\\n    if matches:\\n        for block in matches:\\n            try:\\n                json_obj = json.loads(block.strip())\\n                return json_obj\\n            except json.JSONDecodeError:\\n                continue  # Try the next match if parsing fails\\n\\n    # If no JSON within backticks was found, attempt to parse the entire text as JSON\\n    try:\\n        json_obj = json.loads(text.strip())\\n        return json_obj\\n    except json.JSONDecodeError:\\n        pass  # Parsing the entire text failed\\n\\n    raise ValueError(\"No valid JSON block found in the text.\")\\n'},\n",
              " {'simple\\\\code\\\\custom\\\\first_file_utils.py': 'def create_file(file_path: str, content: str) -> str:\\n    \"\"\"\\n    Creates a new file with the given content.\\n\\n    Args:\\n        file_path: The path to the file to be created.\\n        content: The content to be written into the file.\\n\\n    Returns:\\n        The full path to the written file.\\n    \"\"\"\\n    try:\\n        with open(file_path, \\'w\\') as f:\\n            f.write(content)\\n        return f\"File created at: {file_path}\"\\n    except Exception as e:\\n        return f\"Error creating file: {e}\"\\n\\n\\ndef modify_file(file_path: str, content: str, line_number: int) -> str:\\n    \"\"\"\\n    Modifies a specific line in a file with the given content.\\n\\n    Args:\\n        file_path: The path to the file to be modified.\\n        content: The new content to be written into the specific line.\\n        line_number: The line number to be modified (1-indexed).\\n\\n    Returns:\\n        The full path to the modified file.\\n    \"\"\"\\n    try:\\n        with open(file_path, \\'r\\') as f:\\n            lines = f.readlines()\\n        \\n        if line_number > len(lines):\\n            return f\"Error: Line number {line_number} exceeds file length\"\\n        \\n        lines[line_number - 1] = content + \\'\\\\n\\'\\n\\n        with open(file_path, \\'w\\') as f:\\n            f.writelines(lines)\\n        return f\"File modified at: {file_path}\"\\n    \\n    except FileNotFoundError:\\n          return f\"Error: File not found at {file_path}\"\\n    except Exception as e:\\n        return f\"Error modifying file: {e}\"\\n'},\n",
              " {'simple\\\\code\\\\custom\\\\get_directory_tree.py': '\\ndef get_directory_tree(path: str) -> str:\\n    \"\"\"\\n    Generate a string representation of the directory tree starting from the given path,\\n    while ignoring files and directories matching the specified patterns.\\n\\n    Args:\\n        path: The path to the directory to start from.\\n\\n    Returns:\\n        A string representation of the directory tree.\\n    \"\"\"\\n    import os\\n    import fnmatch\\n\\n    tree = \"\"\\n\\n    ignore_patterns = [\\'interest\\', \\'pyds\\', \\'backup\\', \\'models\\', \\'sdlc\\',\\n                    \\'self_autoCode\\', \\'self_autoCodebase\\', \\'tests\\', \\'to_confirm_tools\\', \\'node_modules\\', \\'gen_ai\\', \\'idea\\', \\'imagev1\\', \\'pretrained\\', \\'prompts\\', \\'.next\\', \\'__pycache__\\']\\n\\n    def is_ignored(path):\\n        if ignore_patterns:\\n            for pattern in ignore_patterns:\\n                if fnmatch.fnmatch(os.path.basename(path), pattern):\\n                    return True\\n        return False\\n\\n    for root, directories, files in os.walk(path):\\n        # Filter out ignored directories\\n        directories[:] = [d for d in directories if not is_ignored(os.path.join(root, d))]\\n        \\n        # Filter out ignored files\\n        files = [f for f in files if not is_ignored(os.path.join(\\n            root, f)) and f.split(\\'.\\')[-1] in [\\'py\\', \\'ts\\', \\'js\\', \\'tsx\\', \\'jsx\\', \\'md\\']]\\n\\n        level = root.replace(path, \\'\\').count(os.sep)\\n        indent = \\' \\' * 4 * (level)\\n        tree += \\'{}{}/\\\\n\\'.format(indent, os.path.basename(root))\\n        subindent = \\' \\' * 4 * (level + 1)\\n        for f in files:\\n            tree += \\'{}{}\\\\n\\'.format(subindent, f)\\n    return tree\\n'},\n",
              " {'simple\\\\code\\\\gui\\\\app.py': '# ./simple/code/gui/app.py:\\n#!/usr/bin/env python3\\n\"\"\"\\nGUI Application for FlexiAI\\n\\nThis module contains the Tkinter-based GUI for interacting with the agent.\\nIt is completely separated from the agents core execution logic.\\n\"\"\"\\n\\nimport os\\nimport tkinter as tk\\nfrom tkinter.scrolledtext import ScrolledText\\nimport threading\\nimport asyncio\\nimport json\\nimport logging\\nimport random\\nimport signal  # Import signal module\\nimport sys  # Import sys module\\nfrom typing import Any, List, Dict, Tuple, Union\\n\\nfrom simple.code import utils\\nfrom simple.code.inference import run_inference, current_client  # Import current_client\\nfrom simple.code.utils import code_corpus, colored_print, Fore\\nfrom simple.code.logging_config import setup_logging\\nfrom simple.code.agent_executor import AgentExecutor\\nfrom simple.code.system_prompts import MD_HEADING, load_message_template\\n# AgentInteractionManager is instantiated within AgentExecutor if not provided\\nfrom simple.agent_interactions import AgentInteractionManager\\n\\nsetup_logging()\\n\\n\\ndef create_labeled_scrolled_text(parent: tk.Frame, label_text: str, height: int, bg_color: str) -> ScrolledText:\\n    \"\"\"\\n    Helper function to create a label and a ScrolledText widget.\\n    \"\"\"\\n    label = tk.Label(parent, text=label_text)\\n    label.pack(anchor=\"w\")\\n    text_area = ScrolledText(parent, height=height, bg=bg_color)\\n    text_area.pack(fill=tk.BOTH, expand=True, pady=5)\\n    return text_area\\n\\n\\nclass FlexiAgentApp:\\n    def __init__(self, root: tk.Tk) -> None:\\n        self.root = root\\n        self.root.title(\"FlexiAI\")\\n        self.set_app_icon()\\n        self.summary = \"\"\\n        self.text_history = []\\n        self.history_index = None\\n        self.model_var = tk.StringVar(value=\"llama3.2\")  # Default model\\n        self.full_prompt = \"\"\\n        self.auto_prompt_set = False\\n        self.first_check = False\\n\\n        # Conversation history\\n        self.conversation_history: List[Dict[str, str]] = []\\n        self.conversation_history_enabled = tk.BooleanVar(value=True)\\n        self.conversation_turns = 3  # Turns before summarizing\\n\\n        # Instantiate the AgentExecutor and its AgentInteractionManager\\n        # If you want the GUI to manage the Tk root for the simulation, pass it here\\n        self.agent_manager = AgentInteractionManager()\\n        self.agent_manager.tk_root = self.root  # Let the manager use the main Tk root\\n        self.agent_executor = AgentExecutor(\\n            model_name=self.model_var.get(),\\n            agent_manager=self.agent_manager\\n        )\\n        # Also assign the model_var to agent_executor for updates (if needed, though decide_execution already reads it)\\n        self.agent_executor.model_var = self.model_var\\n\\n        self.build_gui()\\n        # Bind the up key for input history\\n        self.input_text_area.bind(\"<Up>\", self.on_up_key)\\n        # Bind the window close button to the on_closing method\\n        self.root.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\\n        # Start the input check loop\\n        self.check_input()\\n\\n    def build_gui(self) -> None:\\n        frame = tk.Frame(self.root, padx=10, pady=10)\\n        frame.pack(fill=tk.BOTH, expand=True)\\n\\n        # Right frame: Interaction and Action Output areas\\n        right_frame = tk.Frame(frame, bg=\"lightgray\")\\n        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH,\\n                         expand=True, padx=(10, 0), pady=(0, 5))\\n\\n        tk.Label(right_frame, text=\"Interaction Output\", bg=\"lightgray\", font=(\\n            \"Helvetica\", 12, \"bold\")).grid(row=0, column=0, sticky=\"w\", padx=5, pady=(0, 2))\\n        self.interaction_output_area = ScrolledText(\\n            right_frame, state=tk.DISABLED, bg=\"white\", width=40, height=10)  # Adjusted height\\n        self.interaction_output_area.grid(\\n            row=1, column=0, padx=5, pady=5, sticky=\"nsew\")\\n\\n        tk.Label(right_frame, text=\"Action Output\", bg=\"lightgray\", font=(\\n            \"Helvetica\", 12, \"bold\")).grid(row=2, column=0, sticky=\"w\", padx=5, pady=(10, 2))\\n        self.action_output_area = ScrolledText(\\n            right_frame, state=tk.DISABLED, bg=\"white\", width=40, height=10)  # Adjusted height\\n        self.action_output_area.grid(\\n            row=3, column=0, padx=5, pady=5, sticky=\"nsew\")\\n\\n        right_frame.grid_rowconfigure(1, weight=1)\\n        right_frame.grid_rowconfigure(3, weight=1)\\n        right_frame.grid_columnconfigure(0, weight=1)\\n\\n        # Left frame: Input, Output, Scratchpad, and History\\n        left_frame = tk.Frame(frame)\\n        left_frame.pack(side=tk.LEFT, fill=tk.BOTH,\\n                        expand=True, padx=(0, 5), pady=(0, 5))\\n\\n        # Configuration Panel (Model, Directory, Tips)\\n        config_panel = tk.Frame(left_frame, bd=1, relief=tk.SUNKEN)\\n        config_panel.pack(fill=tk.X, pady=(0, 10))\\n        self.build_config_panel(config_panel)\\n\\n        # Input Area\\n        self.build_input_area(left_frame)\\n\\n        # Output Area\\n        self.build_output_area(left_frame)\\n\\n        # Scratchpad Area\\n        self.build_scratchpad_area(left_frame)\\n\\n        # History Area\\n        history_panel = tk.Frame(left_frame)\\n        history_panel.pack(fill=tk.BOTH, expand=True, pady=(5, 0))\\n        self.build_history_area(history_panel)\\n        self.build_conversation_history_toggle(history_panel)\\n\\n    def build_config_panel(self, parent: tk.Frame) -> None:\\n        \"\"\"Builds the top configuration panel.\"\"\"\\n        tk.Label(parent, text=\"Model:\").grid(\\n            row=0, column=0, sticky=\"w\", padx=5, pady=2)\\n        # TODO: Populate dropdown with available models dynamically if possible\\n        model_options = [\"llama3.1\", \"llama3\",\\n            \"codellama\", \"mistral\"]  # Example models\\n        self.model_dropdown = tk.OptionMenu(\\n            parent, self.model_var, *model_options)\\n        self.model_dropdown.grid(row=0, column=1, sticky=\"ew\", padx=5, pady=2)\\n\\n        tk.Label(parent, text=\"Work Dir:\").grid(\\n            row=1, column=0, sticky=\"w\", padx=5, pady=2)\\n        self.codebase_path_entry = tk.Entry(parent)\\n        self.codebase_path_entry.grid(\\n            row=1, column=1, padx=5, pady=2, sticky=\"ew\")\\n        # TODO: Add browse button?\\n\\n        tk.Label(parent, text=\"Tips:\").grid(\\n            row=2, column=0, sticky=\"nw\", padx=5, pady=2)\\n        self.tips_entry = tk.Text(parent, height=2)  # Reduced height\\n        self.tips_entry.grid(row=2, column=1, padx=5, pady=2, sticky=\"ew\")\\n\\n        parent.grid_columnconfigure(1, weight=1)\\n\\n    def build_input_area(self, parent: tk.Frame) -> None:\\n        input_frame = tk.Frame(parent)\\n        input_frame.pack(fill=tk.X, pady=(0, 5))  # Changed pack options\\n        tk.Label(input_frame, text=\"Enter your request:\").pack(anchor=\"w\")\\n        self.input_text_area = tk.Text(\\n            input_frame, height=4)  # Adjusted height\\n        self.input_text_area.pack(fill=tk.X, expand=True)\\n        button_frame = tk.Frame(parent)\\n        button_frame.pack(fill=tk.X, pady=(0, 5))\\n        self.submit_button = tk.Button(\\n            button_frame, text=\"Submit\", command=self.submit_text)\\n        self.submit_button.pack(side=tk.LEFT, padx=(0, 5))\\n        self.cancel_button = tk.Button(\\n            button_frame, text=\"Cancel\", command=self.cancel_inference)\\n        self.cancel_button.pack(side=tk.LEFT)\\n        # Add simulation button if manager exists\\n        if self.agent_manager:\\n            self.simulation_button = tk.Button(\\n                button_frame, text=\"Show Simulation\", command=self.agent_manager.launch_agent_interaction)\\n            self.simulation_button.pack(side=tk.RIGHT)\\n            # Link button to manager instance for enabling/disabling\\n            self.agent_manager.launch_button = self.simulation_button\\n\\n    def build_output_area(self, parent: tk.Frame) -> None:\\n        output_frame = tk.Frame(parent)\\n        output_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 5))\\n        self.output_text_area = create_labeled_scrolled_text(\\n            output_frame, \"Output:\", 8, \"lightyellow\")  # Adjusted height\\n\\n    def build_scratchpad_area(self, parent: tk.Frame) -> None:\\n        scratchpad_frame = tk.Frame(parent)\\n        scratchpad_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 5))\\n        self.agent_scratchpad_text_area = create_labeled_scrolled_text(\\n            scratchpad_frame, \"Scratchpad:\", 8, \"lightgray\")  # Adjusted height\\n\\n    def build_history_area(self, parent: tk.Frame) -> None:\\n        # history_frame = tk.Frame(parent)\\n        # history_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 5))\\n        self.user_history = create_labeled_scrolled_text(\\n            parent, \"User Input History:\", 5, \"lightblue\")  # Adjusted height\\n        self.user_history.config(state=tk.DISABLED)\\n\\n    def build_conversation_history_toggle(self, parent: tk.Frame) -> None:\\n        toggle_frame = tk.Frame(parent)\\n        # Place at bottom of history panel\\n        toggle_frame.pack(fill=tk.X, side=tk.BOTTOM, pady=(5, 0))\\n        self.history_checkbutton = tk.Checkbutton(\\n            toggle_frame, text=\"Enable Conversation History (Memory)\", variable=self.conversation_history_enabled)\\n        self.history_checkbutton.pack(side=tk.LEFT)\\n\\n    def update_interaction_output(self, role: str, content: str) -> None:\\n        \"\"\"Appends formatted text to the interaction output area.\"\"\"\\n        if not self.root or not self.interaction_output_area.winfo_exists():\\n            return  # Avoid errors if window closed\\n        try:\\n            formatted_text = f\"Role: {role}\\\\nContent:\\\\n{content}\\\\n{\\'-\\'*40}\\\\n\"\\n            self.interaction_output_area.config(state=tk.NORMAL)\\n            self.interaction_output_area.insert(tk.END, formatted_text)\\n            self.interaction_output_area.see(tk.END)\\n            self.interaction_output_area.config(state=tk.DISABLED)\\n        except tk.TclError as e:\\n            logging.warning(\\n                f\"Failed to update interaction output (window might be closing): {e}\")\\n\\n    def set_app_icon(self) -> None:\\n        \"\"\"Sets the application icon.\"\"\"\\n        icon_path = \\'simple/gag/icon.png\\'  # Define icon path\\n        # Check if icon exists, create dummy if not (optional)\\n        if not os.path.exists(icon_path):\\n            logging.warning(\\n                f\"Icon file not found at {icon_path}. Skipping icon setting.\")\\n             # You could create a placeholder or use a default icon here if needed.\\n            return\\n        try:\\n            icon = tk.PhotoImage(file=icon_path)\\n            self.root.iconphoto(True, icon)\\n            logging.info(f\"Loaded application icon from {icon_path}\")\\n        except tk.TclError as e:\\n            # This can happen if Tkinter imaging libs aren\\'t fully available (e.g., for PNG)\\n            # or if the file is corrupted.\\n            logging.error(\\n                f\"Failed to load application icon \\'{icon_path}\\': {e}. Is the image format supported?\")\\n        except Exception as e:\\n            logging.error(\\n                f\"An unexpected error occurred while loading icon \\'{icon_path}\\': {e}\")\\n\\n    def run_async_in_thread(self, coroutine: Any, callback: Any) -> None:\\n        \"\"\"Runs an asyncio coroutine in a separate thread and calls callback with result.\"\"\"\\n        def worker() -> None:\\n            loop = asyncio.new_event_loop()\\n            asyncio.set_event_loop(loop)\\n            try:\\n                result = loop.run_until_complete(coroutine)\\n            except Exception as e:\\n                logging.exception(\"Error in asynchronous coroutine\")\\n                result = f\"Error: {str(e)}\"  # Pass error as result\\n            finally:\\n                loop.close()\\n                # Schedule callback in the main Tkinter thread\\n                if self.root.winfo_exists():  # Check if root window still exists\\n                    self.root.after(0, callback, result)\\n\\n        threading.Thread(target=worker, daemon=True).start()\\n\\n    def cancel_inference(self):\\n        \"\"\"Cancels the ongoing inference if possible.\"\"\"\\n        global current_client\\n        if current_client:\\n            logging.info(\"Attempting to cancel inference...\")\\n            colored_print(\"Attempting to cancel inference...\", Fore.YELLOW)\\n            current_client.cancel()\\n            # Re-enable submit button\\n            self.submit_button.config(state=tk.NORMAL)\\n        else:\\n            logging.info(\"No active inference client to cancel.\")\\n            colored_print(\"No active inference to cancel.\", Fore.YELLOW)\\n\\n    def submit_text(self) -> None:\\n        \"\"\"Handles the submission of user input.\"\"\"\\n        user_input = self.input_text_area.get(\"1.0\", \"end-1c\").strip()\\n        if not user_input:\\n            logging.warning(\"Submit clicked with empty input.\")\\n            return\\n\\n        self.submit_button.config(state=tk.DISABLED)  # Disable submit button\\n        self.cancel_button.config(state=tk.NORMAL)  # Enable cancel button\\n\\n        # Build the full prompt including context (codebase, tips)\\n        self.full_prompt = self.build_full_prompt(user_input)\\n\\n        # Prepare message history for the agent\\n        # Start with base system prompt + summary\\n        messages = load_message_template(\\'base\\', self.summary)\\n\\n        # Add conversation history if enabled\\n        if self.conversation_history_enabled.get() and self.conversation_history:\\n            history_context = \"\\\\n\".join(\\n                [f\"User: {turn[\\'user\\']}\\\\nAssistant: {turn[\\'assistant\\']}\"\\n                 for turn in self.conversation_history]\\n            )\\n            # Add history context to the system message (or as separate messages?)\\n            # Adding to system message might exceed context limits faster.\\n            # Let\\'s try adding as prior turns (limiting length might be needed)\\n            # Limit history to keep prompt size manageable\\n            history_limit = 2  # Keep last N turns\\n            limited_history = self.conversation_history[-history_limit:]\\n            for turn in limited_history:\\n                messages.append({\\'role\\': \\'user\\', \\'content\\': turn[\\'user\\']})\\n                messages.append(\\n                     {\\'role\\': \\'assistant\\', \\'content\\': turn[\\'assistant\\']})\\n            logging.info(\\n                f\"Added {len(limited_history)} turns of conversation history to prompt.\")\\n\\n        # Add the current user request\\n        current_user_prompt = {\\'role\\': \\'user\\', \\'content\\': self.full_prompt}\\n        messages.append(current_user_prompt)\\n\\n        # --- Logging and UI updates ---\\n        # Display combined context + prompt in interaction output for clarity\\n        full_input_for_log = \"\\\\n---\\\\n\".join(\\n            [f\"Role: {m[\\'role\\']}\\\\n{m[\\'content\\']}\" for m in messages])\\n        self.update_interaction_output(\\n            \"Agent Input Context\", full_input_for_log)\\n\\n        # Update user input history display\\n        self.update_user_input_history(user_input)  # Pass current input\\n\\n        # Clear previous outputs\\n        self.output_text_area.delete(\"1.0\", tk.END)\\n        self.agent_scratchpad_text_area.delete(\"1.0\", tk.END)\\n\\n        # Reset input history navigation index\\n        self.history_index = None\\n        # Reset auto-fill flag\\n        self.auto_prompt_set = False\\n\\n        # --- Start Agent Execution ---\\n        # Note: decide_execution now expects the *list* of messages, not just the last user prompt dict\\n        self.run_async_in_thread(\\n            self.agent_executor.agent_execution(  # Pass the full message list\\n                messages, self.output_text_area, self.agent_scratchpad_text_area, self.root\\n            ),\\n            self.handle_response\\n        )\\n\\n    def handle_response(self, response_tuple: Union[Tuple[str, str], str]) -> None:\\n        \"\"\"Callback function to handle the response from the agent executor.\"\"\"\\n\\n        # Check if the response indicates an error from run_async_in_thread itself\\n        if isinstance(response_tuple, str) and response_tuple.startswith(\"Error:\"):\\n            final_response = response_tuple\\n            action_log = \"Execution failed within the async thread.\"\\n            logging.error(f\"Async execution failed: {final_response}\")\\n        elif isinstance(response_tuple, tuple) and len(response_tuple) == 2:\\n            final_response, action_log = response_tuple\\n            logging.info(\"Agent execution completed.\")\\n        else:\\n            final_response = \"Received unexpected response format from agent execution.\"\\n            action_log = f\"Unexpected response: {response_tuple}\"\\n            logging.error(final_response)\\n\\n        # Update conversation history\\n        # Get the user input that triggered this response (it\\'s the last one in text_history)\\n        last_user_input = self.text_history[-1] if self.text_history else \"\"\\n        self.conversation_history.append(\\n            {\"user\": last_user_input, \"assistant\": final_response}\\n        )\\n\\n        # Display final response in the main output area\\n        self.output_text_area.insert(tk.END, final_response)\\n\\n        # Log assistant\\'s final response\\n        self.update_interaction_output(\"Assistant (Final)\", final_response)\\n\\n        # Display the action log\\n        self.action_output_area.config(state=tk.NORMAL)\\n        self.action_output_area.delete(\"1.0\", tk.END)\\n        self.action_output_area.insert(tk.END, action_log)\\n        self.action_output_area.config(state=tk.DISABLED)\\n\\n        # Summarize conversation periodically if enabled\\n        if self.conversation_history_enabled.get() and len(self.conversation_history) > 0 and \\\\\\n           len(self.conversation_history) % self.conversation_turns == 0:\\n            self.summarize_conversation()\\n\\n        # Re-enable submit button, disable cancel button\\n        self.submit_button.config(state=tk.NORMAL)\\n        self.cancel_button.config(state=tk.DISABLED)\\n\\n    def build_full_prompt(self, user_input: str) -> str:\\n        \"\"\"Builds the user\\'s part of the prompt including codebase and tips.\"\"\"\\n        codebase_path = self.codebase_path_entry.get().strip()\\n        base_code = \"\"\\n        # Check if it\\'s a valid directory\\n        if codebase_path and os.path.isdir(codebase_path):\\n            logging.info(f\"Reading codebase from: {codebase_path}\")\\n            try:\\n                # Use utils.code_corpus to get formatted code strings\\n                base_code_list = utils.code_corpus(codebase_path)\\n                base_code = \"\\\\n\".join(base_code_list)\\n                if not base_code:\\n                    logging.warning(\\n                        f\"No Python files found or read in directory: {codebase_path}\")\\n            except Exception as e:\\n                logging.error(\\n                    f\"Error reading codebase from {codebase_path}: {e}\")\\n                # Optionally inform user in UI? For now, just log.\\n        elif codebase_path:\\n            logging.warning(\\n                f\"Provided codebase path is not a valid directory: {codebase_path}\")\\n\\n        tips = self.tips_entry.get(\"1.0\", \"end-1c\").strip()\\n\\n        # Construct the prompt, adding sections only if they have content\\n        prompt_parts = []\\n        # Always include the request\\n        prompt_parts.append(f\"{MD_HEADING} Request:\\\\n{user_input}\")\\n\\n        if base_code:\\n            # Add codebase at the beginning if present\\n            prompt_parts.insert(\\n                0, f\"{MD_HEADING} Codebase Context:\\\\n{base_code}\")\\n\\n        if tips:\\n            # Add tips at the end if present\\n            prompt_parts.append(f\"\\\\n{MD_HEADING} Additional Tips:\\\\n{tips}\")\\n\\n        full_prompt_text = \"\\\\n\\\\n\".join(prompt_parts)\\n        # logging.debug(f\"Built full prompt:\\\\n{full_prompt_text[:500]}...\") # Log snippet\\n        return full_prompt_text\\n\\n    def update_user_input_history(self, current_input: str) -> None:\\n        \"\"\"Adds current input to history list and updates the display.\"\"\"\\n        if not current_input:\\n            return\\n\\n        self.text_history.append(current_input)\\n        # Optional: Limit history size\\n        max_history = 50\\n        if len(self.text_history) > max_history:\\n            self.text_history = self.text_history[-max_history:]\\n\\n        # Update the ScrolledText widget\\n        try:\\n            if self.user_history.winfo_exists():\\n                self.user_history.config(state=tk.NORMAL)\\n                self.user_history.delete(\"1.0\", tk.END)\\n                # Display history with numbers, newest last\\n                history_display = \"\\\\n\".join(\\n                    f\"{i+1}: {entry}\" for i, entry in enumerate(reversed(self.text_history)))\\n                self.user_history.insert(tk.END, history_display)\\n                self.user_history.config(state=tk.DISABLED)\\n                self.user_history.see(\"1.0\")  # Scroll to top (most recent)\\n        except tk.TclError as e:\\n            logging.warning(f\"Failed to update user history display: {e}\")\\n\\n    def on_up_key(self, event) -> str:\\n        \"\"\"Handler for the Up arrow key to recall previous input.\"\"\"\\n        if not self.text_history:\\n            return \"break\"  # Indicates Tkinter should not process further\\n\\n        if self.history_index is None:\\n            # Start from the most recent entry\\n            self.history_index = len(self.text_history) - 1\\n        elif self.history_index > 0:\\n            # Move to the previous (older) entry\\n            self.history_index -= 1\\n        else:\\n            # Already at the oldest entry\\n            return \"break\"\\n\\n        # Display the recalled history item\\n        self.input_text_area.delete(\"1.0\", tk.END)\\n        self.input_text_area.insert(\\n            \"1.0\", self.text_history[self.history_index])\\n        return \"break\"  # Prevent default Up key behavior\\n\\n    def check_input(self) -> None:\\n        \"\"\"\\n        Periodically checks for user inactivity and potentially auto-fills a prompt.\\n        (Currently set to check every 60 seconds).\\n        \"\"\"\\n        # This auto-fill feature might be intrusive, consider making it optional or less frequent.\\n        check_interval_ms = 60000  # 60 seconds\\n\\n        current_input = \"\"\\n        try:\\n            if self.input_text_area.winfo_exists():\\n                current_input = self.input_text_area.get(\\n                    \"1.0\", \"end-1c\").strip()\\n        except tk.TclError:\\n            logging.info(\"Input area check skipped, widget destroyed.\")\\n            return  # Stop checking if widget is gone\\n\\n        # Check if input is empty AND submit button is enabled (i.e., not currently processing)\\n        submit_state = self.submit_button.cget(\\n            \\'state\\') if self.submit_button.winfo_exists() else \\'disabled\\'\\n\\n        if not current_input and not self.auto_prompt_set and submit_state == tk.NORMAL:\\n            if self.first_check:\\n                logging.info(\\n                    f\"No input detected for {check_interval_ms // 1000}s. Scheduling auto-prompt.\")\\n                # Run auto-fill in a separate thread to avoid blocking GUI\\n                threading.Thread(target=lambda: asyncio.run(\\n                    self.auto_fill_and_submit_prompt()), daemon=True).start()\\n            else:\\n                # First time checking after startup or submission, just set the flag\\n                self.first_check = True\\n                logging.info(\"Input check: Initial check passed.\")\\n        elif current_input:\\n            # If user has typed something, reset the first_check flag\\n            self.first_check = False\\n            self.auto_prompt_set = False  # Also reset auto_prompt flag if user interacted\\n            # logging.debug(\"Input check: User input detected.\") # Less verbose log\\n\\n        # Schedule the next check only if the root window still exists\\n        if self.root.winfo_exists():\\n            self.root.after(check_interval_ms, self.check_input)\\n\\n    async def auto_fill_and_submit_prompt(self) -> None:\\n        \"\"\"Generates a suggestion and fills the input area, then schedules submission.\"\"\"\\n        if self.auto_prompt_set:  # Avoid race conditions\\n            return\\n\\n        logging.info(\"Generating auto-prompt suggestion...\")\\n        self.auto_prompt_set = True  # Set flag early\\n\\n        # Define the base path for code corpus more robustly\\n        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(\\n            os.path.abspath(__file__))))  # Should point to project root\\n        simple_dir = os.path.join(base_dir, \\'simple\\')\\n\\n        base_code = \"\"\\n        if os.path.isdir(simple_dir):\\n            base_code = \"\".join(code_corpus(simple_dir))\\n        else:\\n            logging.warning(\\n                f\"Auto-prompt: \\'simple\\' directory not found at {simple_dir}\")\\n\\n        if not base_code:\\n            base_prompt_content = \"Suggest a creative task for an AI agent.\"\\n        else:\\n            # Keep prompt concise\\n            base_prompt_content = f\"## Codebase Context (summary):\\\\n{base_code[:1000]}...\\\\n\\\\n## Task: Suggest one specific, actionable improvement for this codebase that could be performed by an AI agent using tools or Python code.\"\\n\\n        prompt_messages = load_message_template(\\n            \\'base\\', \\'\\')  # Use base template for suggestion\\n        prompt_messages.append(\\n            {\\'role\\': \\'user\\', \\'content\\': base_prompt_content})\\n\\n        # Run inference to get the suggestion\\n        # Use a temporary scratchpad update mechanism if desired, or None\\n        suggestion = run_inference(\\n            # No scratchpad update for this internal call\\n            prompt_messages, None, self.root, self.model_var.get())\\n\\n        if not suggestion or suggestion.startswith(\"Error\"):\\n            logging.error(\\n                f\"Failed to generate auto-prompt suggestion: {suggestion}\")\\n            self.auto_prompt_set = False  # Reset flag on failure\\n            return\\n\\n        # Function to update GUI from this thread via root.after\\n        def _update_gui_with_suggestion():\\n            if not self.root.winfo_exists():\\n                return\\n            try:\\n                self.input_text_area.delete(\"1.0\", tk.END)\\n                self.input_text_area.insert(tk.END, suggestion.strip())\\n                logging.info(f\"Auto-filled prompt: {suggestion.strip()}\")\\n                 # Schedule the actual submission after a short delay (e.g., 1 second)\\n                self.root.after(1000, self.submit_text)\\n            except tk.TclError as e:\\n                logging.warning(\\n                    f\"Failed to update GUI with auto-prompt (window closed?): {e}\")\\n                self.auto_prompt_set = False  # Reset flag\\n\\n        # Schedule the GUI update in the main thread\\n        if self.root.winfo_exists():\\n            self.root.after(0, _update_gui_with_suggestion)\\n        else:\\n            self.auto_prompt_set = False  # Reset flag if root gone\\n\\n    def summarize_conversation(self) -> None:\\n        \"\"\"Summarizes the recent conversation history.\"\"\"\\n        if not self.conversation_history:\\n            logging.info(\"No conversation history to summarize.\")\\n            return\\n\\n        # Get recent turns for summary\\n        history_to_summarize = self.conversation_history[-self.conversation_turns:]\\n        history_text = \"\\\\n\".join([\\n            f\"User: {turn[\\'user\\']}\\\\nAssistant: {turn[\\'assistant\\']}\"\\n            for turn in history_to_summarize\\n        ])\\n\\n        # Prepare the summarization prompt\\n        summary_prompt_messages = load_message_template(\\n            \\'summary\\', \\'\\')  # Get summary system prompt\\n        summary_prompt_messages.append(\\n            {\\'role\\': \\'user\\', \\'content\\': history_text})\\n\\n        logging.info(\\n            f\"Summarizing last {len(history_to_summarize)} conversation turns...\")\\n        self.update_interaction_output(\\n            \"System\", \"Summarizing conversation...\")  # UI feedback\\n\\n        def handle_summary_result(summary_result: Union[Tuple[str, str], str]) -> None:\\n            \"\"\"Callback to process the summary result.\"\"\"\\n            if isinstance(summary_result, str) and summary_result.startswith(\"Error:\"):\\n                summary = f\"Failed to summarize conversation: {summary_result}\"\\n                logging.error(summary)\\n            elif isinstance(summary_result, tuple):\\n                summary = summary_result[0]  # First element is the response\\n                logging.info(f\"Conversation summarized successfully.\")\\n                 # Replace history with summary (or keep summary separate?)\\n                 # For now, just store the summary and clear the history used for it.\\n                self.summary = summary\\n                 # Clear only the summarized part, keep newer turns if any added during summary\\n                self.conversation_history = self.conversation_history[len(\\n                     history_to_summarize):]\\n                logging.info(f\"Stored summary: {self.summary[:100]}...\")\\n                self.update_interaction_output(\\n                     \"System\", f\"Conversation Summary:\\\\n{self.summary}\")\\n            else:\\n                summary = \"Unexpected summary result format.\"\\n                logging.error(summary)\\n\\n            # Update UI or internal state with the summary\\n            # (self.summary is updated internally)\\n\\n        # Use run_async_in_thread for the summarization call\\n        # Note: summarize_conversation is called within handle_response, which is already in the main thread.\\n        # However, the actual inference for summarization should be async.\\n        # decide_execution is not ideal here as it triggers the full agent loop.\\n        # We need a way to just run inference. Let\\'s call run_inference directly.\\n\\n        # Define the async task for summarization inference\\n        async def _summarize_task():\\n            return run_inference(summary_prompt_messages, None, self.root, self.model_var.get())\\n\\n        # Run the summarization task in a background thread\\n        self.run_async_in_thread(_summarize_task(), handle_summary_result)\\n\\n    def on_closing(self) -> None:\\n        \"\"\"Handles the event when the user closes the Tkinter window.\"\"\"\\n        logging.info(\"Application window closing...\")\\n        print(\"Shutting down FlexiAI...\")\\n\\n        # 1. Cancel any ongoing inference\\n        self.cancel_inference()\\n\\n        # 2. Signal the Pygame simulation thread to stop\\n        if self.agent_manager:\\n            self.agent_manager.stop_game()\\n\\n        # 3. Optionally wait for threads? (Daemon threads should exit automatically)\\n        #    For cleaner shutdown, one might join non-daemon threads here.\\n\\n        # 4. Destroy the Tkinter window and exit the main loop\\n        self.root.destroy()\\n        print(\"FlexiAI shutdown complete.\")\\n\\n\\n# Global reference to the app instance for the signal handler\\n_app_instance = None\\n\\n\\ndef signal_handler(app: FlexiAgentApp, sig, frame):\\n    \"\"\"Handles termination signals like SIGINT (Ctrl+C).\"\"\"\\n    print(f\"\\\\nReceived signal {sig}. Shutting down...\")\\n    logging.info(f\"Received signal {sig}. Initiating shutdown.\")\\n    if app:\\n        # Trigger the same closing logic as the window close button\\n        app.on_closing()\\n    else:\\n        # If app instance isn\\'t available, try basic cleanup\\n        if \\'agent_manager\\' in locals() or \\'agent_manager\\' in globals():\\n            try:\\n                 # Attempt to access manager if possible (less reliable)\\n                manager = locals().get(\\'agent_manager\\') or globals().get(\\'agent_manager\\')\\n                if manager:\\n                     manager.stop_game()\\n            except Exception as e:\\n                print(\\n                    f\"Error stopping agent manager during signal handling: {e}\")\\n        # Exit the process\\n        sys.exit(0)\\n\\n\\ndef main() -> None:\\n    \"\"\"Main function to initialize and run the application.\"\"\"\\n    global _app_instance\\n\\n    # Set up signal handling for graceful shutdown on Ctrl+C\\n    # Pass the app instance to the handler using a lambda\\n    def main_signal_handler(sig, frame):\\n        if _app_instance:\\n             signal_handler(_app_instance, sig, frame)\\n        else:\\n            print(\\n                 f\"\\\\nReceived signal {sig} before app fully initialized. Exiting.\")\\n            sys.exit(0)\\n\\n    signal.signal(signal.SIGINT, main_signal_handler)\\n    signal.signal(signal.SIGTERM, main_signal_handler)  # Handle termination signal\\n\\n    # Initialize Tkinter\\n    root = tk.Tk()\\n    app = FlexiAgentApp(root)\\n    _app_instance = app  # Store instance globally for signal handler\\n\\n    # Start the Tkinter main loop\\n    try:\\n        root.mainloop()\\n    except KeyboardInterrupt:\\n        # This might catch Ctrl+C if signal handler doesn\\'t exit fully\\n        print(\"\\\\nKeyboardInterrupt caught in mainloop. Forcing exit.\")\\n         # Ensure cleanup if signal handler didn\\'t complete\\n        if _app_instance:\\n            _app_instance.on_closing()  # Attempt cleanup again\\n        sys.exit(1)\\n\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n'},\n",
              " {'simple\\\\image\\\\imgen.py': 'from diffusers import StableDiffusionPipeline  # type: ignore\\nimport torch  # type: ignore\\nfrom PIL import Image # type: ignore\\nimport base64\\nfrom io import BytesIO\\nfrom google import genai\\nfrom google.genai import types # type: ignore\\nfrom dotenv import load_dotenv # type: ignore\\nimport os\\nload_dotenv()\\n\\n\\nclient = genai.Client(api_key=os.getenv(\\'gemini_key\\'))\\n\\n\\ndef edit_image(path, new_path):\\n    image = Image.open(\\'gemini-native-image.png\\')\\n\\n\\n    text_input = (\\'Hi, This is a picture of a robot.\\'\\n                \\'Can you add a robot dog next to it?\\',)\\n\\n    response = client.models.generate_content(\\n        model=\"gemini-2.0-flash-exp-image-generation\",\\n        contents=[text_input, image],\\n        config=types.GenerateContentConfig(\\n            response_modalities=[\\'Text\\', \\'Image\\']\\n        )\\n    )\\n\\n    for part in response.candidates[0].content.parts:\\n        if part.text is not None:\\n            print(part.text)\\n        elif part.inline_data is not None:\\n            image = Image.open(BytesIO(part.inline_data.data))\\n            image.save(new_path)\\n            image.show()\\n\\n\\ndef interpert_image(path):\\n   # Image to text\\n    image = Image.open(path)\\n    response = client.models.generate_content_stream(\\n        model=\"gemini-2.0-flash\",\\n        contents=[image, \"Tell me about this image\"]\\n    )\\n    final = \\'\\'\\n    for chunk in response:\\n        section = chunk.text\\n        print(section, end=\"\")\\n        final += section\\n\\n    return final\\n\\n\\n\\ndef generate_image(prompt: str,\\n                   output_filename: str = \"generated_image.png\",\\n                   model_name: str = \"runwayml/stable-diffusion-v1-5\",\\n                   transparent: bool = False) -> None:\\n    \"\"\"\\n    Generates an image from the given prompt using Stable Diffusion,\\n    optionally processes the image to remove a white background (making it transparent),\\n    saves it to the specified file, and displays the image.\\n\\n    Args:\\n        prompt (str): The text prompt for image generation.\\n        output_filename (str, optional): The file path to save the generated image.\\n            Defaults to \"generated_image.png\".\\n        model_name (str, optional): The Hugging Face model identifier for Stable Diffusion.\\n            Defaults to \"runwayml/stable-diffusion-v1-5\".\\n        transparent (bool, optional): If True, processes the image to remove near-white pixels,\\n            making the background transparent. Defaults to False.\\n    \"\"\"\\n    try:\\n        # text to image\\n        contents = prompt\\n\\n        response = client.models.generate_content(\\n            model=\"gemini-2.0-flash-exp-image-generation\",\\n            contents=contents,\\n            config=types.GenerateContentConfig(\\n                response_modalities=[\\'Text\\', \\'Image\\']\\n            )\\n        )\\n\\n        for part in response.candidates[0].content.parts:\\n          if part.text is not None:\\n            print(part.text)\\n          elif part.inline_data is not None:\\n            image = Image.open(BytesIO((part.inline_data.data)))\\n            image.save(output_filename)\\n            image.show()\\n    except Exception as e:\\n        # Load the Stable Diffusion pipeline\\n        pipe = StableDiffusionPipeline.from_pretrained(model_name)\\n        # Uncomment the following line to use GPU (if available)\\n        # pipe = pipe.to(\"cuda\")  # or \"cpu\" if needed\\n\\n        # Generate an image from the prompt\\n        image = pipe(prompt).images[0]\\n\\n        if transparent:\\n            # Convert the image to RGBA to support transparency.\\n            image = image.convert(\"RGBA\")\\n            # Process each pixel: if the RGB values are near white, set its alpha to 0.\\n            new_data = []\\n            for pixel in image.getdata():\\n                # Define threshold for white (adjust threshold as needed)\\n                if pixel[0] > 240 and pixel[1] > 240 and pixel[2] > 240:\\n                    # Replace near-white with transparent\\n                    new_data.append((255, 255, 255, 0))\\n                else:\\n                    new_data.append(pixel)\\n            image.putdata(new_data)\\n\\n        # Save and display the image\\n        image.save(output_filename)\\n        # image.show()\\n\\n\\n# Example usage:\\nif __name__ == \"__main__\":\\n    # Generate a Python logo for Pygame with transparent background:\\n    generate_image(\"Minimalistic Python logo for pygame with transparent background\",\\n                   output_filename=\"pygame_python_logo.png\",\\n                   transparent=True)\\n\\n    # Generate a toolbox icon for Pygame with transparent background:\\n    generate_image(\"Modern toolbox icon for pygame with transparent background\",\\n                   output_filename=\"pygame_toolbox.png\",\\n                   transparent=True)\\n    \\n    generate_image(\"Generate a logo for an app called \",\\n                   output_filename=\"pygame_toolbox.png\",\\n                   transparent=True)\\n'},\n",
              " {'simple\\\\image\\\\imregen.py': 'import numpy as np\\nfrom diffusers import StableDiffusionImg2ImgPipeline # type: ignore\\nimport torch # type: ignore\\nfrom PIL import Image # type: ignore\\nimport numpy as np # type: ignore\\n\\nimport os\\nimport glob\\nimport torch\\nfrom torch.utils.data import Dataset\\nfrom torchvision import transforms\\nfrom PIL import Image\\n\\n# Load the pipeline\\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\")\\n\\npath_loc = \"simple/gag/generated_image.png\"\\n# Load the image, convert to RGB, then to a numpy array\\ninit_image = Image.Image(path_loc).convert(\"RGB\")\\n# init_image_np = np.array(init_image)\\n\\n\\n# Open the image using PIL\\n# torch_tensor = Image.open(path_loc)\\nprint(\"Type after opening with PIL:\", type(\\n    init_image))  # <class \\'PIL.Image.Image\\'>\\n\\n# # Convert the PIL image to a NumPy array\\n# torch_tensor = np.array(pil_image)\\n# print(\"Type after converting to NumPy array:\", type(\\n#     torch_tensor), torch_tensor.shape)  # <class \\'numpy.ndarray\\'>\\n\\n# # Convert the NumPy array to a PyTorch tensor\\n# torch_tensor = torch.from_numpy(np_image)\\n# print(\"Type after converting to PyTorch tensor:\", type(\\n#     torch_tensor), torch_tensor.shape)  # <class \\'torch.Tensor\\'>\\n\\n\\n# Define your prompt\\nprompt = \"A futuristic cityscape at sunset with vibrant neon lights\"\\n\\n# Generate the new image\\nresult = pipe(prompt=prompt, init_image=init_image).images[0]\\n\\n# Save and display the resulting image\\nresult.save(\"img2img_result.png\")\\nresult.show()\\n'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "code_corpus('simple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_registry.update({code_corpus.__name__: code_corpus})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'execute_bash_command': <function function_call.execute_bash_command(command: str) -> str>,\n",
              " 'write_custom_python_file': <function function_call.write_custom_python_file(file_path: str, code: str) -> str>,\n",
              " 'create_file': <function first_file_utils.create_file(file_path: str, content: str) -> str>,\n",
              " 'modify_file': <function first_file_utils.modify_file(file_path: str, content: str, line_number: int) -> str>,\n",
              " 'get_directory_tree': <function get_directory_tree.get_directory_tree(path: str) -> str>,\n",
              " 'code_corpus': <function __main__.code_corpus(directory: str) -> List[str]>}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tools_docs():\n",
        "    \"\"\"Gets list of tools that can be used\"\"\"\n",
        "    base = [types.FunctionDeclaration.from_callable(\n",
        "        callable=func, client=client).to_json_dict()\n",
        "        for name, func in tool_registry.items() if name in ['write_custom_python_file', 'get_directory_tree', 'execute_bash_command', 'create_file', 'modify_file']]\n",
        "    # base.append(types.FunctionDeclaration.from_callable(\n",
        "    #     callable=get_tools_docs, client=client).to_json_dict())\n",
        "    base.append(types.FunctionDeclaration.from_callable(\n",
        "        callable=code_corpus, client=client).to_json_dict())\n",
        "\n",
        "    return base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'description': '\\nExecutes a bash command provided as a string.\\n\\nArgs:\\n    command: The command to execute.\\n             - If string: Treated as a single command line, split by shlex.\\n\\nReturns:\\n    str: The standard output of the command if successful.\\n\\nRaises:\\n    ValueError: If the command format is unsupported or invalid.\\n    Exception: If the command fails to execute (non-zero exit code or other error).\\n              The exception message includes stderr.\\n',\n",
              "  'name': 'execute_bash_command',\n",
              "  'parameters': {'properties': {'command': {'type': 'STRING'}},\n",
              "   'required': ['command'],\n",
              "   'type': 'OBJECT'}},\n",
              " {'description': '\\nWrites the provided Python code to a file within the \\'simple/code/custom\\' directory.\\nPrevents path traversal attacks.\\n\\nArgs:\\n    file_path (str): The desired filename (e.g., \"my_tool.py\"). Should not contain path separators.\\n    code (str): The Python code content to write.\\n\\nReturns:\\n    str: The full path to the written file if successful.\\n\\nRaises:\\n    ValueError: If file_path is invalid or attempts path traversal.\\n    OSError: If file writing fails.\\n',\n",
              "  'name': 'write_custom_python_file',\n",
              "  'parameters': {'properties': {'file_path': {'type': 'STRING'},\n",
              "    'code': {'type': 'STRING'}},\n",
              "   'required': ['file_path', 'code'],\n",
              "   'type': 'OBJECT'}},\n",
              " {'description': '\\nCreates a new file with the given content.\\n\\nArgs:\\n    file_path: The path to the file to be created.\\n    content: The content to be written into the file.\\n\\nReturns:\\n    The full path to the written file.\\n',\n",
              "  'name': 'create_file',\n",
              "  'parameters': {'properties': {'file_path': {'type': 'STRING'},\n",
              "    'content': {'type': 'STRING'}},\n",
              "   'required': ['file_path', 'content'],\n",
              "   'type': 'OBJECT'}},\n",
              " {'description': '\\nModifies a specific line in a file with the given content.\\n\\nArgs:\\n    file_path: The path to the file to be modified.\\n    content: The new content to be written into the specific line.\\n    line_number: The line number to be modified (1-indexed).\\n\\nReturns:\\n    The full path to the modified file.\\n',\n",
              "  'name': 'modify_file',\n",
              "  'parameters': {'properties': {'file_path': {'type': 'STRING'},\n",
              "    'content': {'type': 'STRING'},\n",
              "    'line_number': {'type': 'INTEGER'}},\n",
              "   'required': ['file_path', 'content', 'line_number'],\n",
              "   'type': 'OBJECT'}},\n",
              " {'description': '\\nGenerate a string representation of the directory tree starting from the given path,\\nwhile ignoring files and directories matching the specified patterns.\\n\\nArgs:\\n    path: The path to the directory to start from.\\n\\nReturns:\\n    A string representation of the directory tree.\\n',\n",
              "  'name': 'get_directory_tree',\n",
              "  'parameters': {'properties': {'path': {'type': 'STRING'}},\n",
              "   'required': ['path'],\n",
              "   'type': 'OBJECT'}},\n",
              " {'description': '\\nBuilds a corpus by reading all Python, Typescript and Javascript files in the directory.\\n',\n",
              "  'name': 'code_corpus',\n",
              "  'parameters': {'properties': {'directory': {'type': 'STRING'}},\n",
              "   'required': ['directory'],\n",
              "   'type': 'OBJECT'}}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_tools_docs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lwLZrmW5zR_P"
      },
      "outputs": [],
      "source": [
        "n = 0\n",
        "custom_functions = tool_registry\n",
        "\n",
        "async def run(prompt, modality=\"TEXT\", tools=None):\n",
        "\n",
        "    global n\n",
        "\n",
        "    if tools is None:\n",
        "\n",
        "        tools = []\n",
        "\n",
        "    config = {\n",
        "\n",
        "\n",
        "        \"tools\": tools,\n",
        "\n",
        "\n",
        "        \"response_modalities\": [modality]}\n",
        "\n",
        "    async with client.aio.live.connect(model=model_name, config=config) as session:\n",
        "\n",
        "        display.display(display.Markdown(prompt))\n",
        "\n",
        "        display.display(display.Markdown('-------------------------------'))\n",
        "\n",
        "        await session.send(input=prompt, end_of_turn=True)\n",
        "\n",
        "        async for response in session.receive():\n",
        "\n",
        "            logger.debug(str(response))\n",
        "\n",
        "            if text := response.text:\n",
        "\n",
        "                display.display(display.Markdown(text))\n",
        "\n",
        "                continue\n",
        "\n",
        "            server_content = response.server_content\n",
        "\n",
        "            if server_content is not None:\n",
        "\n",
        "                handle_server_content(server_content)\n",
        "\n",
        "                continue\n",
        "\n",
        "            tool_call = response.tool_call\n",
        "\n",
        "            if tool_call is not None:\n",
        "\n",
        "                await handle_tool_call(session, tool_call, custom_functions)\n",
        "\n",
        "        n = n+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "guiding = open('simple/Guide.md', 'r', encoding='utf-8').read()\n",
        "corpus = code_corpus('simple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QmB_4XPOslyA"
      },
      "outputs": [],
      "source": [
        "# prompt = f\"\"\"\n",
        "#   Look at the project in the simple directory:\n",
        "#   {corpus}\n",
        "\n",
        "#   Follow this guide:\n",
        "#   {guiding}\n",
        "\n",
        "#   Implement the Clarity of Output Areas\n",
        "#   \"\"\"\n",
        "\n",
        "# prompt = f\"\"\"\n",
        "#   I need you to do a couple of things for me.\n",
        "\n",
        "#   1. Write a python function that will take file_path, content and create a file (add docstring and type hints).\n",
        "#   2. Write a python function that will take file_path, content, line to edit and modify the file (add docstring and type hints).\n",
        "  \n",
        "#   \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\katle\\AppData\\Local\\Temp\\ipykernel_18428\\1366411343.py:20: ExperimentalWarning: The live API is experimental and may change in future versions.\n",
            "  async with client.aio.live.connect(model=model_name, config=config) as session:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "\n",
              "  I need you to do a couple of things for me.\n",
              "\n",
              "  Look at the ollama documentation. Find out how I can implement an ai agent using python or javascript. Show me the code to an AI agent that will be able to start and complete tasks or projects on its own.\n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Okay"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              ", I will start by searching the Ollama documentation to find information about implementing AI"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " agents using Python or JavaScript. Then, I'll try to provide code examples"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " for an agent that can start and complete tasks or projects. I'll focus on demonstrating the core logic and interaction with Ollama.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "``` python\n",
              "concise_search(\"ollama documentation python agent\", max_num_results=3)\n",
              "concise_search(\"ollama documentation javascript agent\", max_num_results=3)\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "```\n",
              "Looking up information on Google Search.\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "```\n",
              "Looking up information on Google Search.\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "The"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " search results provide some useful information. It seems that both Python and JavaScript can be"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " used to build agents with Ollama. The key concepts are using the Ollama"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " library to interact with the language models, and defining \"tools\" for agents to use, such as functions or APIs. There are examples available, particularly for Python"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              ", and also integrations with libraries like Langchain.\n",
              "\n",
              "I'll focus on a basic example to illustrate the core principles. I will create a basic python script"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " to interact with ollama, and use the tool calling capability to perform simple tasks. I will not get into the complexities of creating a full self-starting agent that manages projects on its own in this response, but will demonstrate the basic principles"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              ".\n",
              "\n",
              "Here is the Python code for a basic agent that uses a function tool:\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "``` python\n",
              "import ollama\n",
              "import json\n",
              "\n",
              "def get_current_weather(city):\n",
              "    \"\"\"Simulates fetching the current weather for a city.\"\"\"\n",
              "    if city == \"Toronto\":\n",
              "        return \"The weather in Toronto is sunny with a temperature of 25 degrees Celsius.\"\n",
              "    elif city == \"London\":\n",
              "        return \"The weather in London is cloudy with a temperature of 18 degrees Celsius.\"\n",
              "    else:\n",
              "        return \"Weather information for \" + city + \" is not available.\"\n",
              "\n",
              "def run_agent():\n",
              "    tools = [{\n",
              "        'type': 'function',\n",
              "        'function': {\n",
              "            'name': 'get_current_weather',\n",
              "            'description': 'Get the current weather for a city',\n",
              "            'parameters': {\n",
              "                'type': 'object',\n",
              "                'properties': {\n",
              "                    'city': {\n",
              "                        'type': 'string',\n",
              "                        'description': 'The name of the city',\n",
              "                    },\n",
              "                },\n",
              "                'required': ['city'],\n",
              "            },\n",
              "        },\n",
              "    }]\n",
              "    messages = [{'role': 'user', 'content': 'What is the weather in Toronto and London?'}]\n",
              "    response = ollama.chat(\n",
              "        model='llama3.1',\n",
              "        messages=messages,\n",
              "        tools=tools\n",
              "    )\n",
              "\n",
              "    if 'tool_calls' in response['message']:\n",
              "        tool_calls = response['message']['tool_calls']\n",
              "        for tool_call in tool_calls:\n",
              "            function_name = tool_call['function']['name']\n",
              "            arguments = json.loads(tool_call['function']['arguments'])\n",
              "            if function_name == 'get_current_weather':\n",
              "                city = arguments['city']\n",
              "                weather_info = get_current_weather(city)\n",
              "                messages.append({\n",
              "                    'role': 'tool',\n",
              "                    'tool_call_id': tool_call['id'],\n",
              "                    'content': weather_info\n",
              "                })\n",
              "        second_response = ollama.chat(\n",
              "           model='llama3.1',\n",
              "            messages=messages,\n",
              "        )\n",
              "        print(second_response['message']['content'])\n",
              "\n",
              "\n",
              "    else:\n",
              "        print(response['message']['content'])\n",
              "\n",
              "run_agent()\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "```\n",
              "Traceback (most recent call last):\n",
              "  File \"<string>\", line 325, in <module>\n",
              "ModuleNotFoundError: No module named 'ollama'\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Ah"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              ", it appears I made a mistake and assumed the `ollama` library was"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " already available in the environment. It is not. I need to install it first"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " before running the python code. Let me correct that and add a check if the library is available.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "``` python\n",
              "import subprocess\n",
              "import sys\n",
              "\n",
              "def install_package(package):\n",
              "    try:\n",
              "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
              "    except subprocess.CalledProcessError as e:\n",
              "       print(f\"Error installing {package}: {e}\")\n",
              "       return False\n",
              "    return True\n",
              "\n",
              "\n",
              "try:\n",
              "    import ollama\n",
              "    print(\"Ollama library is already installed\")\n",
              "except ImportError:\n",
              "    print(\"Ollama library is not installed, attempting to install\")\n",
              "    if install_package(\"ollama\"):\n",
              "      import ollama\n",
              "      print(\"Ollama library successfully installed\")\n",
              "    else:\n",
              "      print(\"Failed to install ollama library. Please install it manually.\")\n",
              "      exit()\n",
              "\n",
              "import json\n",
              "\n",
              "def get_current_weather(city):\n",
              "    \"\"\"Simulates fetching the current weather for a city.\"\"\"\n",
              "    if city == \"Toronto\":\n",
              "        return \"The weather in Toronto is sunny with a temperature of 25 degrees Celsius.\"\n",
              "    elif city == \"London\":\n",
              "        return \"The weather in London is cloudy with a temperature of 18 degrees Celsius.\"\n",
              "    else:\n",
              "        return \"Weather information for \" + city + \" is not available.\"\n",
              "\n",
              "def run_agent():\n",
              "    tools = [{\n",
              "        'type': 'function',\n",
              "        'function': {\n",
              "            'name': 'get_current_weather',\n",
              "            'description': 'Get the current weather for a city',\n",
              "            'parameters': {\n",
              "                'type': 'object',\n",
              "                'properties': {\n",
              "                    'city': {\n",
              "                        'type': 'string',\n",
              "                        'description': 'The name of the city',\n",
              "                    },\n",
              "                },\n",
              "                'required': ['city'],\n",
              "            },\n",
              "        },\n",
              "    }]\n",
              "    messages = [{'role': 'user', 'content': 'What is the weather in Toronto and London?'}]\n",
              "    response = ollama.chat(\n",
              "        model='llama3.1',\n",
              "        messages=messages,\n",
              "        tools=tools\n",
              "    )\n",
              "\n",
              "    if 'tool_calls' in response['message']:\n",
              "        tool_calls = response['message']['tool_calls']\n",
              "        for tool_call in tool_calls:\n",
              "            function_name = tool_call['function']['name']\n",
              "            arguments = json.loads(tool_call['function']['arguments'])\n",
              "            if function_name == 'get_current_weather':\n",
              "                city = arguments['city']\n",
              "                weather_info = get_current_weather(city)\n",
              "                messages.append({\n",
              "                    'role': 'tool',\n",
              "                    'tool_call_id': tool_call['id'],\n",
              "                    'content': weather_info\n",
              "                })\n",
              "        second_response = ollama.chat(\n",
              "           model='llama3.1',\n",
              "            messages=messages,\n",
              "        )\n",
              "        print(second_response['message']['content'])\n",
              "\n",
              "\n",
              "    else:\n",
              "        print(response['message']['content'])\n",
              "\n",
              "run_agent()\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "```\n",
              "Ollama library is not installed, attempting to install\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "```\n",
              "Traceback (most recent call last):\n",
              "  File \"<string>\", line 342, in <module>\n",
              "  File \"<string>\", line 330, in install_package\n",
              "  File \"subprocess.py\", line 408, in check_call\n",
              "    retcode = call(*popenargs, **kwargs)\n",
              "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
              "  File \"subprocess.py\", line 389, in call\n",
              "    with Popen(*popenargs, **kwargs) as p:\n",
              "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
              "  File \"subprocess.py\", line 1026, in __init__\n",
              "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
              "  File \"subprocess.py\", line 1824, in _execute_child\n",
              "    and os.path.dirname(executable)\n",
              "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
              "  File \"<frozen posixpath>\", line 152, in dirname\n",
              "ModuleNotFoundError: No module named 'ollama'\n",
              "\n",
              "During handling of the above exception, another exception occurred:\n",
              "\n",
              "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Okay"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              ", it appears the installation failed because the `ollama` library cannot be found"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " by the pip installer due to a path issue. This is because pip is trying"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " to find the ollama executable and not the python package. My attempt to install the `ollama` package using pip programmatically did not work as expected."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " I should have just used `pip install ollama` directly. Since I can't reliably install it using pip from within this environment, I'll skip"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " that part and assume that the user has installed the `ollama` library themselves. I will proceed to run the agent code as it was in the previous attempt, without the install step. If the user doesn't have the library,"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " they'll have to install it manually before running the code.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "``` python\n",
              "import json\n",
              "import ollama\n",
              "\n",
              "\n",
              "def get_current_weather(city):\n",
              "    \"\"\"Simulates fetching the current weather for a city.\"\"\"\n",
              "    if city == \"Toronto\":\n",
              "        return \"The weather in Toronto is sunny with a temperature of 25 degrees Celsius.\"\n",
              "    elif city == \"London\":\n",
              "        return \"The weather in London is cloudy with a temperature of 18 degrees Celsius.\"\n",
              "    else:\n",
              "        return \"Weather information for \" + city + \" is not available.\"\n",
              "\n",
              "def run_agent():\n",
              "    tools = [{\n",
              "        'type': 'function',\n",
              "        'function': {\n",
              "            'name': 'get_current_weather',\n",
              "            'description': 'Get the current weather for a city',\n",
              "            'parameters': {\n",
              "                'type': 'object',\n",
              "                'properties': {\n",
              "                    'city': {\n",
              "                        'type': 'string',\n",
              "                        'description': 'The name of the city',\n",
              "                    },\n",
              "                },\n",
              "                'required': ['city'],\n",
              "            },\n",
              "        },\n",
              "    }]\n",
              "    messages = [{'role': 'user', 'content': 'What is the weather in Toronto and London?'}]\n",
              "    response = ollama.chat(\n",
              "        model='llama3.1',\n",
              "        messages=messages,\n",
              "        tools=tools\n",
              "    )\n",
              "\n",
              "    if 'tool_calls' in response['message']:\n",
              "        tool_calls = response['message']['tool_calls']\n",
              "        for tool_call in tool_calls:\n",
              "            function_name = tool_call['function']['name']\n",
              "            arguments = json.loads(tool_call['function']['arguments'])\n",
              "            if function_name == 'get_current_weather':\n",
              "                city = arguments['city']\n",
              "                weather_info = get_current_weather(city)\n",
              "                messages.append({\n",
              "                    'role': 'tool',\n",
              "                    'tool_call_id': tool_call['id'],\n",
              "                    'content': weather_info\n",
              "                })\n",
              "        second_response = ollama.chat(\n",
              "           model='llama3.1',\n",
              "            messages=messages,\n",
              "        )\n",
              "        print(second_response['message']['content'])\n",
              "\n",
              "\n",
              "    else:\n",
              "        print(response['message']['content'])\n",
              "\n",
              "run_agent()\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "```\n",
              "Traceback (most recent call last):\n",
              "  File \"<string>\", line 326, in <module>\n",
              "ModuleNotFoundError: No module named 'ollama'\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------------------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "  I need you to do a couple of things for me.\n",
        "\n",
        "  Look at the ollama documentation. Find out how I can implement an ai agent using python or javascript. Show me the code to an AI agent that will be able to start and complete tasks or projects on its own.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "tools = [\n",
        "    {'google_search': {}},\n",
        "    {'code_execution': {}},\n",
        "    {'function_declarations': get_tools_docs()}\n",
        "]\n",
        "\n",
        "await run(prompt, tools=tools, modality=\"TEXT\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Get_started_LiveAPI_tools.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "pyds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
